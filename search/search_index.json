{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyMLup","text":""},{"location":"#table-of-contents","title":"Table of contents","text":"<ul> <li>Quickstart</li> <li>Description of the configuration file</li> <li>Description of the application life cycle</li> <li>Storages</li> <li>Binarizers</li> <li>Data Transformers</li> <li>Web app architectures</li> <li>Web app API</li> <li>Description of the Python Interface</li> <li>Description of the bash commands</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>It's library for easy and fast run ML in production. </p> <p>All you need is to deliver the model file and config to the server (in fact, the config is not necessary) \ud83d\ude43</p> <p>PyMLup is a modern way to run machine learning models in production. The market time has been reduced to a minimum. This library eliminates the need to write your own web applications with machine learning models and copy application code. It is enough to have a machine learning model to launch a web application with one command.</p> <ul> <li>It's library learning only clean python;</li> <li>Use FastApi in web app backend;</li> </ul> <p>Work tested with machine learning model frameworks (links to tests):</p> <ul> <li>scikit-learn&gt;=1.2.0,&lt;1.3.0</li> <li>tensorflow&gt;=2.0.0,&lt;3.0.0</li> <li>lightgbm&gt;=4.0.0,&lt;5.0.0</li> <li>torch&gt;=2.0.0,&lt;3.0.0</li> <li>onnx&gt;=1.0.0,&lt;2.0.0</li> <li>onnxruntime&gt;=1.0.0,&lt;2.0.0</li> </ul> <p>Support and tested with machine learning libraries:</p> <ul> <li>numpy&gt;=1.0.0,&lt;2.0.0</li> <li>pandas&gt;=2.0.0,&lt;3.0.0</li> <li>joblib&gt;=1.2.0,&lt;1.3.0</li> <li>tf2onnx&gt;=1.0.0,&lt;2.0.0</li> <li>skl2onnx&gt;=1.0.0,&lt;2.0.0</li> <li>jupyter==1.0.0</li> </ul> <p>The easiest way to try:</p> <pre><code>pip install pymlup\nmlup run -m /path/to/my/model.onnx\n</code></pre>"},{"location":"#useful-links","title":"Useful links","text":"<ul> <li>Docs;</li> <li>Examples;</li> <li>Tests models;</li> </ul>"},{"location":"#how-its-work","title":"How it's work","text":"<ol> <li>You are making your machine learning model. Optional: you are making mlup config for your model.</li> <li>You deliver your model to server. Optional: you deliver your config to server.</li> <li>Installing pymlup to your server and libraries for model.</li> <li>Run web app from your model or your config \ud83d\ude43</li> </ol>"},{"location":"#requirements","title":"Requirements","text":"<p>Python 3.7+</p> <ul> <li>PyMLup stands on the shoulders of giants FastAPI for the web parts. </li> <li>Additionally, you need to install the libraries that your model uses.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pymlup\n</code></pre> <p>You will also can install with ml backend library:</p> <pre><code>pip install \"pymlup[scikit-learn]\"  # For scikit-learn\npip install \"pymlup[lightgbm]\"      # For microsoft lightgbm\npip install \"pymlup[tensorflow]\"    # For tensorflow\npip install \"pymlup[torch]\"         # For torch\npip install \"pymlup[onnx]\"          # For onnx models: torch, tensorflow, sklearn, etc...\n</code></pre>"},{"location":"#examples","title":"Examples","text":""},{"location":"#examples-code","title":"Examples code","text":"<pre><code>import mlup\n\nclass MyAnyModelForExample:\n    def predict(self, X):\n        return X\n\nml_model = MyAnyModelForExample()\n\n\nup = mlup.UP(ml_model=ml_model)\n# Need call up.ml.load(), for analyze your model\nup.ml.load()\n# If you want testing your web app, you can run in daemon mode\n# You can open browser http://localhost:8009/docs\nup.run_web_app(daemon=True)\n\nimport requests\nresponse = requests.post('http://0.0.0.0:8009/predict', json={'X': [[1, 2, 3], [4, 5, 6]]})\nprint(response.json())\n\nup.stop_web_app()\n</code></pre> <p>You can check work model by config, without web application. * <code>predict</code> - Get model predict as inner arguments as in web app. * <code>predict_from</code> - As <code>predict</code> method, but not use data transformer before call model predict. * <code>async_predict</code> - Asynchronous version of the <code>predict</code> method.</p> <pre><code>import mlup\nimport numpy\n\nclass MyAnyModelForExample:\n    def predict(self, X):\n        return X\n\nml_model = MyAnyModelForExample()\nup = mlup.UP(ml_model=ml_model)\nup.ml.load()\n\nup.predict(X=[[1, 2, 3], [4, 5, 6]])\nup.predict_from(X=numpy.array([[1, 2, 3], [4, 5, 6]]))\nawait up.async_predict(X=[[1, 2, 3], [4, 5, 6]])\n</code></pre>"},{"location":"#save-ready-application-to-disk","title":"Save ready application to disk","text":""},{"location":"#make-default-config","title":"Make default config","text":"<p>If path endswith to json, make json config, else yaml config.</p> <pre><code>import mlup\nmlup.generate_default_config('path_to_yaml_config.yaml')\n</code></pre>"},{"location":"#from-config","title":"From config","text":"<p>You can save ready config to disk, but you need set local storage and path to model file in server. In folder can there are many files, mask need for filter exactly our model file</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel  # This stub class\nfrom mlup import constants\n\nup = mlup.UP(ml_model=EmptyModel())\nup.conf.storage_type = constants.StorageType.disk\nup.conf.storage_kwargs = {\n    'path_to_files': 'path/to/model/file/in/model_name.modelextension',\n    'file_mask': 'model_name.modelextension',\n}\nup.to_yaml(\"path_to_yaml_config.yaml\")\nup.to_json(\"path_to_json_config.json\")\n\n# After in server\nup = mlup.UP.load_from_yaml(\"path_to_yaml_config.yaml\", load_model=True)\nup.run_web_app()\n</code></pre>"},{"location":"#from-pickle","title":"From pickle","text":"<p>If you make pickle/joblib file your mlup with model, don't need to change storage type, because your model there is in your pickle/joblib file.</p> <pre><code>import pickle\nimport mlup\nfrom mlup.ml.empty import EmptyModel  # This stub class\n\nup = mlup.UP(ml_model=EmptyModel())\n\n# You can create pickle file\nwith open('path_to_pickle_file.pckl', 'wb') as f:\n    pickle.dump(up, f)\n\n# After in server\nwith open('path_to_pickle_file.pckl', 'rb') as f:\n    up = pickle.load(f)\nup.ml.load()\nup.run_web_app()\n</code></pre>"},{"location":"#change-config","title":"Change config","text":"<p>If you can change model settings (See Description of the application life cycle), need call <code>up.ml.load_model_settings()</code>.</p> <pre><code>import mlup\n\nclass MyAnyModelForExample:\n    def predict(self, X):\n        return X\n\nml_model = MyAnyModelForExample()\n\nup = mlup.UP(\n    ml_model=ml_model,\n    conf=mlup.Config(port=8011)\n)\nup.ml.load()\nup.conf.auto_detect_predict_params = False\nup.ml.load_model_settings()\n</code></pre>"},{"location":"#examples-server-commands","title":"Examples server commands","text":""},{"location":"#mlup-run","title":"mlup run","text":"<p>You can run web application from model, config, pickle up object. Bash command mlup run making this.</p> <p>See <code>mlup run --help</code> or Description of the bash commands for full docs.</p>"},{"location":"#from-model","title":"From model","text":"<pre><code>mlup run -m /path/to/your/model.extension\n</code></pre> <p>This will run code something like this: </p> <pre><code>import mlup\nfrom mlup import constants\n\nup = mlup.UP(\n    conf=mlup.Config(\n        storage_type=constants.StorageType.disk,\n        storage_kwargs={\n            'path_to_files': '/path/to/your/model.extension',\n            'files_mask': r'.+',\n        },\n    )\n)\nup.ml.load()\nup.run_web_app()\n</code></pre> <p>You change config attributes in this mode. For this, you can add arguments <code>--up.&lt;config_attribute_name&gt;=new_value</code>.  (For more examples see <code>mlup run --help</code> or Description of the bash commands).</p>"},{"location":"#from-config_1","title":"From config","text":"<pre><code>mlup run -c /path/to/your/config.yaml\n# or mlup run -ct json -c /path/to/your/config.json\n</code></pre> <p>This will run code something like this:</p> <pre><code>import mlup\n\nup = mlup.UP.load_from_yaml(conf_path='/path/to/your/config.yaml', load_model=True)\nup.run_web_app()\n</code></pre>"},{"location":"#from-mlupup-picklejoblib-object","title":"From mlup.UP pickle/joblib object","text":"<pre><code>mlup run -b /path/to/your/up_object.pckl\n# or mlup run -bt joblib -b /path/to/your/up_object.joblib\n</code></pre> <p>This will run code something like this:</p> <pre><code>import pickle\n\nwith open('/path/to/your/up_object.pckl', 'rb') as f:\n    up = pickle.load(f)\nup.run_web_app()\n</code></pre>"},{"location":"#mlup-make-app","title":"mlup make-app","text":"<p>This command making <code>.py</code> file with mlup web application and your model, config, pickle up object or with default settings.</p> <p>See <code>mlup make-app --help</code> or Description of the bash commands for full docs.</p>"},{"location":"#with-default-settings","title":"With default settings","text":"<pre><code>mlup make-app example_without_data_app.py\n</code></pre> <p>This command is making something like this:</p> <pre><code># example_without_data_app.py\nimport mlup\n\n\n# You can load the model yourself and pass it to the \"ml_model\" argument.\n# up = mlup.UP(ml_model=my_model, conf=mlup.Config())\nup = mlup.UP(\n    conf=mlup.Config(\n        # Set your config, for work model and get model.\n        # You can use storage_type and storage_kwargs for auto_load model from storage.\n    )\n)\nup.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn example_app:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n</code></pre> <p>And you can write your settings and run web application:</p> <pre><code>python3 example_without_data_app.py\n</code></pre>"},{"location":"#with-only-model","title":"With only model","text":"<pre><code>mlup make-app -ms /path/to/my/model.onnx example_without_data_app.py\n</code></pre> <p>This command is making something like this:</p> <pre><code># example_without_data_app.py\nimport mlup\nfrom mlup import constants\n\n\nup = mlup.UP(\n    conf=mlup.Config(\n        # Set your config, for work model and get model.\n        storage_type=constants.StorageType.disk,\n        storage_kwargs={\n            'path_to_files': '/path/to/my/model.onnx',\n            'files_mask': 'model.onnx',\n        },\n    )\n)\nup.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn example_app:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n\n</code></pre> <p>And you can run web application:</p> <pre><code>python3 example_without_data_app.py\n</code></pre>"},{"location":"#with-only-config","title":"With only config","text":"<pre><code>mlup make-app -cs /path/to/my/config.yaml example_without_data_app.py\n</code></pre> <p>This command is making something like this:</p> <pre><code># example_without_data_app.py\nimport mlup\n\n\nup = mlup.UP.load_from_yaml('/path/to/my/config.yaml', load_model=False)\nup.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn example_app:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n</code></pre> <p>And you can run web application:</p> <pre><code>python3 example_without_data_app.py\n</code></pre>"},{"location":"#with-only-binary-up-object","title":"With only binary UP object","text":"<pre><code>mlup make-app -bs /path/to/my/up.pickle example_without_data_app.py\n</code></pre> <p>This command is making something like this:</p> <pre><code># example_without_data_app.py\nimport pickle\n\n\nwith open('/path/to/my/up.pickle', 'rb') as f:\n    up = pickle.load(f)\n\nif not up.ml.loaded:\n    up.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn example_app:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n</code></pre> <p>And you can run web application:</p> <pre><code>python3 example_without_data_app.py\n</code></pre>"},{"location":"#mlup-validate-config","title":"mlup validate-config","text":"<p>This command use for validation your config. This command have alpha version and need finalize.</p> <p>See <code>mlup validate-config --help</code> or Description of the bash commands for full docs.</p> <pre><code>mlup validate-config /path/to/my/conf.yaml\n</code></pre>"},{"location":"#web-application-interface","title":"Web application interface","text":"<p>By default, web application starting on http://localhost:8009 and have api docs.</p> <p>See Web app API for more details. </p>"},{"location":"#interactive-api-docs","title":"Interactive API docs","text":"<p>Now go to http://localhost:8009/docs.</p> <p>You will see the automatic interactive API documentation (provided by Swagger UI):</p>"},{"location":"#api-points","title":"Api points","text":""},{"location":"#health","title":"/health","text":"<p>Use for check health web application.</p> <p>HTTP's methods: HEAD, OPTIONS, GET</p>   ##### Return JSON  ```{'status': 200}``` and status code is 200."},{"location":"#info","title":"/info","text":"<p>Use for get model and application information. If set debug=True in config, return full config. </p> <p>HTTP's methods: GET</p>   ##### Return JSON:  <pre><code>{\n  \"model_info\": {\n    \"name\": \"MyFirstMLupModel\",\n    \"version\": \"1.0.0.0\",\n    \"type\": \"sklearn\",\n    \"columns\": null\n  },\n  \"web_app_info\": {\n    \"version\": \"1.0.0.0\",\n  }\n}\n</code></pre>   If set in config `debug=True`, return another json, almost complete config. But no sensitive data.   <pre><code>{\n  \"web_app_config\": {\n    \"host\": \"localhost\",\n    \"port\": 8009,\n    \"web_app_version\": \"1.0.0.0\",\n    \"column_validation\": false,\n    \"custom_column_pydantic_model\": null,\n    \"mode\": \"mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture\",\n    \"max_queue_size\": 100,\n    \"ttl_predicted_data\": 60,\n    \"ttl_client_wait\": 30.0,\n    \"min_batch_len\": 10,\n    \"batch_worker_timeout\": 1.0,\n    \"is_long_predict\": false,\n    \"show_docs\": true,\n    \"debug\": true,\n    \"throttling_max_requests\": null,\n    \"throttling_max_request_len\": null,\n    \"timeout_for_shutdown_daemon\": 3.0,\n    \"item_id_col_name\": \"mlup_item_id\"\n  },\n  \"model_config\": {\n    \"name\": \"MyFirstMLupModel\",\n    \"version\": \"1.0.0.0\",\n    \"type\": \"sklearn\",\n    \"columns\": null,\n    \"predict_method_name\": \"predict\",\n    \"auto_detect_predict_params\": true,\n    \"storage_type\": \"mlup.ml.storage.memory.MemoryStorage\",\n    \"binarization_type\": \"auto\",\n    \"use_thread_loop\": true,\n    \"max_thread_loop_workers\": true,\n    \"data_transformer_for_predict\": \"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\",\n    \"data_transformer_for_predicted\": \"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\",\n    \"dtype_for_predict\": null\n  }\n}\n</code></pre>"},{"location":"#predict","title":"/predict","text":"<p>Use for call predict in model.</p> <p>HTTP's methods: POST</p>   ##### Requests body data:  <pre><code>{\n  \"data_for_predict\": [\n    \"input_data_for_obj_1\",\n    \"input_data_for_obj_2\",\n    \"input_data_for_obj_3\"\n  ]\n}\n</code></pre>   Key `data_for_predict` is default key for inner data. In config by default set param `auto_detect_predict_params` is True.  This param activate analyze model predict method, get arguments from and generate API by params.  If `auto_detect_predict_params` found params, he changes `data_for_predict` to finding keys and change API docs.  Example for `scikit-learn` models:  <pre><code>{\n  \"X\": [\n    \"input_data_for_obj_1\",\n    \"input_data_for_obj_2\",\n    \"input_data_for_obj_3\"\n  ]\n}\n</code></pre>   `input_data_for_obj_1` maybe any valid JSON data. These data are run through data transformers from config `data_transformer_for_predict`.  By default, this param is `mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer`.  ##### Return JSON:  <pre><code>{\n  \"predict_result\": [\n    \"predict_result_for_obj_1\",\n    \"predict_result_for_obj_2\",\n    \"predict_result_for_obj_3\"\n  ]\n}\n</code></pre>   `predict_result_for_obj_1` will be valid JSON data. These data, after being predicted by the model, are run through data transformers from config `data_transformer_for_predicted`.  By default, this param is `mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer`."},{"location":"#validation","title":"Validation","text":"<p>This method have validation for inner request data. It's making from config <code>columns</code> and flag <code>column_validation</code>.</p>"},{"location":"#web-application-modes","title":"Web application modes","text":"<p>See Web app architectures for more details. </p> <p>Web application have three works modes: * <code>directly_to_predict</code> - is Default. User request send directly to model. * <code>worker_and_queue</code> - ml model starts in thread worker and take data for predict from queue.    Web application new user requests send to queue and wait result from results queue. * <code>batching</code> - ml model start in thread worker and take data for predict from queue.    But not for one request, but combines data from several requests and sends it in one large array to the model.    Web application new user requests send to queue and wait result from results queue.</p> <p>This param is naming <code>mode</code>.</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\nfrom mlup import constants\n\nup = mlup.UP(\n    ml_model=EmptyModel(),\n    conf=mlup.Config(\n        mode=constants.WebAppArchitecture.worker_and_queue,\n    )\n)\n</code></pre> <p>If your model is light, or you hae many CPU/GPU/RAM, you can run many processes:</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\nfrom mlup import constants\n\nup = mlup.UP(\n    ml_model=EmptyModel(),\n    conf=mlup.Config(\n        mode=constants.WebAppArchitecture.worker_and_queue,\n        uvicorn_kwargs={'workers': 4},\n    )\n)\n</code></pre>"},{"location":"#metrics","title":"Metrics","text":"<p>MLup PyPi download statistics: https://pepy.tech/project/pymlup</p> <p> </p>"},{"location":"bash_commands/","title":"Description of the bash commands","text":""},{"location":"bash_commands/#mlup-validate-config","title":"mlup validate-config","text":"<p>This command allows you to validate your config file for correctness.</p> <pre><code>mlup validate-config /path/to/your/config/file.yaml\nLoad config from /path/to/your/config/file.yaml\nConfig is valid\n\nmlup validate-config --type json /path/to/your/config/file.json\nLoad config from /path/to/your/config/file.json\nConfig is valid\n</code></pre> <p>So far, this command cannot check the validity of the values in the configuration file. The command simply checks that mlup can read and load this config.</p> <p>This is due to the fact that not everything can be validated without running the code to load the model. But I'm working on creating full validation for each field, its valid values, and the cumulative values throughout the entire config.</p> <p>If the config is not valid, this command will exit with exit code 1.</p>"},{"location":"bash_commands/#mlup-run","title":"mlup run","text":"<p>Perhaps the most interesting command, which allows you to run the application directly according to the model without a config.</p> <pre><code>mlup run -m /path/to/my/model.onnx\n</code></pre> <p>But this team is much broader. It can run the application: * by model - <code>mlup run -m /path/to/my/model.onnx</code>; * according to the config - <code>mlup run -c /path/to/my/config.yaml</code>; * according to the pickle file with mlup.UP - <code>mlup run -b /path/to/my/binary.pickle</code>;</p> <p>Also, you can replace any config value on the fly, directly in the arguments.</p> <pre><code>mlup run -c /path/to/my/config.yaml --up.port=8010 --up.use_thread_loop=False\n</code></pre> <p>The order of using the config in this case: * The most priority config from the arguments <code>--up.&lt;conf_name&gt;</code>; * Config from your configuration file; * Default config value;</p> <p>Examples for different data types:</p> <pre><code> mlup run -m /path/tomy/model.onnx \\\n  --up.port=8011 \\\n  --up.batch_worker_timeout=10.0 \\\n  --up.predict_method_name=\\\"__call__\\\" \\\n  --up.use_thread_loop=False \\\n  --up.columns='[{\"name\": \"col\", \"type\": \"list\"}]' \\\n  --up.uvicorn_kwargs='{\"workers\": 4, \"timeout_graceful_shutdown\": 10}'\n</code></pre> <p>You can specify <code>--verbose</code> to get full logging with debug loggers for debugging.</p> <pre><code>mlup run -m /path/to/my/model.onnx --verbose\n</code></pre>"},{"location":"bash_commands/#mlup-make-app","title":"mlup make-app","text":"<p>If the <code>mlup run</code> command is not enough for you, you can create a <code>.py</code> file with your mlup application. In which you can add any of your Python code. For example: add your own API handlers for a web application.</p> <pre><code>mlup make-app /path/to/your/app.py\nApp success created: /path/to/your/app.py\n</code></pre> <p>This command will generate the following code in the file <code>/path/to/your/app.py</code>:</p> <pre><code>import mlup\n\n\n# You can load the model yourself and pass it to the \"ml_model\" argument.\n# up = mlup.UP(ml_model=my_model, conf=mlup.Config())\nup = mlup.UP(\n    conf=mlup.Config(\n        # Set your config, for work model and get model.\n        # You can use storage_type and storage_kwargs for auto_load model from storage.\n    )\n)\nup.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn test-app2:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n</code></pre> <p>You can control which application will be generated using arguments: * <code>mlup make-app -ms /path/to/your/model.onnx /path/to/your/app.py</code> - will add model loading from disk to the application. * <code>mlup make-app -\u0441s /path/to/your/config.yaml /path/to/your/app.py</code> - will add config loading from disk to the application. * <code>mlup make-app -bs /path/to/your/mlupUP.pickle /path/to/your/app.py</code> - will add loading mlup.UP from disk and unpickle to the application.</p> <p>For example, the command <code>mlup make-app -ms /path/to/your/model.onnx /path/to/your/app.py</code> will create the file:</p> <pre><code>import mlup\nfrom mlup import constants\n\n\nup = mlup.UP(\n    conf=mlup.Config(\n        # Set your config, for work model and get model.\n        storage_type=constants.StorageType.disk,\n        storage_kwargs={\n            'path_to_files': '/path/to/your/model.onnx',\n            'files_mask': 'model.onnx',\n        },\n    )\n)\nup.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn test-app2:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n</code></pre> <p>If the file <code>/path/to/your/app.py</code> already exists, you can specify <code>--force</code> to overwrite it. Then the existing file will be deleted and a new one will be created.</p> <p>Just like in <code>mlup run</code>, you can specify your own configuration options via the <code>--up.&lt;conf_name&gt;</code> argument.</p> <pre><code>mlup make-app -ms /path/to/your/model.onnx /path/to/your/app.py --up.port=8010 --up.use_thread_loop=False`\n</code></pre> <p>Will create the following file:</p> <pre><code>import mlup\nfrom mlup import constants\n\n\nup = mlup.UP(\n    conf=mlup.Config(\n        # Set your config, for work model and get model.\n        storage_type=constants.StorageType.disk,\n        storage_kwargs={\n            'path_to_files': '/path/to/your/model.onnx',\n            'files_mask': 'model.onnx',\n        },\n        port=8010,\n        use_thread_loop=False,\n    )\n)\nup.ml.load()\nup.web.load()\n\n# If you want to run the application yourself, or add something else to it, use this variable.\n# Example with uvicorn: uvicorn test-app2:app --host 0.0.0.0 --port 80\napp = up.web.app\n\nif __name__ == '__main__':\n    up.run_web_app()\n</code></pre> <p>You can launch the application through the created file with one command:</p> <pre><code>python /path/to/your/app.py\n</code></pre>"},{"location":"binarizers/","title":"Binarizers","text":"<p>There are a huge number of ways to deliver a machine learning model to the server. But in all cases, you need to deliver some data to this model: a binary pickle object, weights and layers of the neural network, text configuration of the trained model, and others. These are always some files or one file.</p> <p>When your machine learning model application runs, it loads the model into memory using this data.</p> <p>There are many ways to turn a trained model into a file and back again. Only some of them are supported out of the box in mlup:</p> <ul> <li>pickle - docs is here;</li> <li>joblib - docs is here;</li> <li>lightgbm - docs is here;</li> <li>torch all formats - docs is here;</li> <li>tensorflow all formats - docs is here;</li> <li>onnx - docs is here;</li> </ul> <p>You can select the one you need in the configuration using <code>binarization_type</code>. In <code>binarization_type</code> you can specify: * \"auto\" - default. In this case, mlup will try to automatically select a binarizer based on the name of the model file and its contents. If it fails, a ModelLoadError exception will be thrown. * select one of the mlup binarizers and specify it using mlup.constants.BinarizationType. * specify your own binarizer - the full python import line for your binarizer.</p>"},{"location":"binarizers/#auto","title":"Auto","text":"<p>This is the default way to select a binarizer for a model. It uses well-known mlup binarizers, each of which has some knowledge about its model storage format.</p> <p>Based on this knowledge, the binarizer analyzes the model name and the contents of the model source files and returns a probability of the degree of confidence that it can load this model. The binarizer with the greatest confidence and tries to load. And if all binarizers returned confidence 0, then the automatic selection is considered unsuccessful.</p> <p>Not all binarizers participate in the automatic binarizer search, but only: * mlup.ml.binarization.pickle.PickleBinarizer * mlup.ml.binarization.joblib.JoblibBinarizer * mlup.ml.binarization.lightgbm.LightGBMBinarizer * mlup.ml.binarization.torch.TorchBinarizer * mlup.ml.binarization.tensorflow.TensorFlowBinarizer * mlup.ml.binarization.tensorflow.TensorFlowSavedBinarizer * mlup.ml.binarization.onnx.InferenceSessionBinarize * mlup.ml.binarization.onnx.InferenceSessionFullReturnBinarizer</p> <p>If the model is not already loaded into memory, binarizers do not load it into memory for analysis. Instead, they can read up to 100 bytes from the beginning of the file and up to 100 bytes from the end of the file. Therefore, the memory used by the application should not increase when you use automatic binarizer selection.</p> <p>After successful selection, mlup will install the found binarizer in your application config in <code>binarization_type</code>. If your application saves the config after this, then it will no longer contain \u201cauto\u201d, but the selected binarizer. This is done so as not to select a binarizer for the same model several times.</p> <p>IMPORTANT: automatic selection does not guarantee 100% correct determination of the desired binarizer and may make mistakes. Please be careful and check in the logs and config which binarizer mlup chose for you.</p>"},{"location":"binarizers/#mlup-binaraizers","title":"Mlup binaraizers","text":""},{"location":"binarizers/#mlupmlbinarizationpicklepicklebinarizer","title":"mlup.ml.binarization.pickle.PickleBinarizer","text":"<p>Pickle binarizer (<code>mlup.ml.binarization.pickle.PickleBinarizer</code>) is quite simple. All of his code actually converges on a call to <code>pickle.load()</code>.</p>"},{"location":"binarizers/#auto-search","title":"Auto search","text":"<p>To check whether the binarizer can load a model from a file, the binarizer uses the file extension and the first and last bytes of the file.</p> <p>According to pickle documentation, the first and last bytes are always the same. This sign adds 90% confidence.</p> <p>The idea is to check the file for these bytes with similar code:</p> <pre><code>import pickletools\n\nis_pickle_file_confidence: float = 0.0\n\nwith open(\"file.pickle\", \"rb\") as f:\n    start = f.read(1)\n    f.seek(-2, 2)\n    end = f.read()\n    file_bytes = start + end\n    start_opcode = pickletools.code2op.get(file_bytes[0:1].decode('latin-1'))\n    end_opcode = pickletools.code2op.get(file_bytes[-1:].decode('latin-1'))\n    if start_opcode.name == 'PROTO' and end_opcode.name == 'STOP':\n        is_pickle_file_confidence = 0.9\n</code></pre> <p>In addition to this, the file extension will be checked. If it is <code>.pckl</code> or <code>.pkl</code>, confidence increases by 5%.</p> <pre><code>is_pickle_file_confidence: float\n\nif 'file.pickle'.endswith('.pckl') or 'file.pickle'.endswith('.pkl'):\n    is_pickle_file_confidence += 0.05\n</code></pre> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and improve the analysis code.</p>"},{"location":"binarizers/#mlupmlbinarizationjoblibjoblibbinarizer","title":"mlup.ml.binarization.joblib.JoblibBinarizer","text":"<p>Joblib binarizer is a copy of Pickle binarizer, except calling <code>joblib.load()</code> instead of <code>pickle.load()</code>.</p>"},{"location":"binarizers/#auto-search_1","title":"Auto search","text":"<p>Although joblib is involved in automatic selection, it completely copies the pickle analysis method.</p>"},{"location":"binarizers/#mlupmlbinarizationlightgbmlightgbmbinarizer","title":"mlup.ml.binarization.lightgbm.LightGBMBinarizer","text":"<p>The LightGBM binarizer builds a model based on the <code>lightgbm.Booster</code> constructor.</p> <pre><code>import lightgbm as lgb\n\npath: str\nraw_model_data: str\n\nif path:\n    model = lgb.Booster(model_file=path)\nmodel = lgb.Booster(model_str=raw_model_data)\n</code></pre>"},{"location":"binarizers/#auto-search_2","title":"Auto search","text":"<p>LightGBM has a text format for saving settings. mlup reads the first 100 bytes of the settings file and looks for a familiar signature there. In the current implementation, this is a search for a string starting with \"version\" in the first 100 bytes of the file. But in the future this signature may change and become more complex. This will definitely be written about in the documentation. This sign brings 80% confidence.</p> <p>Searching for a signature looks something like this:</p> <pre><code>is_pickle_file_confidence: float = 0.0\n\nwith open('model_path.txt', 'r') as f:\n    file_data = f.read(100)\n*rows, other = file_data.split('\\n', 5)\nif any([r.split('=')[0] == 'version' for r in rows]):\n    is_pickle_file_confidence = 0.8\n\n</code></pre> <p>In addition to this, the file extension will be checked. If it is <code>.txt</code>, confidence increases by 5%.</p> <pre><code>is_pickle_file_confidence: float\n\nif 'file.txt'.endswith('.txt'):\n    is_pickle_file_confidence += 0.05\n</code></pre> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and improve the analysis code.</p>"},{"location":"binarizers/#mlupmlbinarizationtorchtorchbinarizer","title":"mlup.ml.binarization.torch.TorchBinarizer","text":"<p>Python has its own way of saving and loading a model. mlup uses exactly this. You can read more about ways to save and load your model in torch in the documentation.</p> <p>The entire binarizer code can be reduced to approximately the following lines:</p> <pre><code>import torch\n\nwith open('file_path.pth', 'rb') as f:\n    model = torch.load(f)\nmodel.eval()\n</code></pre>"},{"location":"binarizers/#auto-search_3","title":"Auto search","text":"<p>This binarizer is involved in automatic selection. Based solely on our observations, torch models saved in the standard way have the same bytes at the beginning and end of the file. Similar to Pickle binarization.</p> <p>Considering that there is no official confirmation of this observation, this sign only adds 50% confidence. Also, the <code>mlup.ml.binarization.tensorflow.TensorFlowBinarizer</code> binarizer is based on the same feature and the same bytes.</p> <p>The following code does this check:</p> <pre><code>is_pickle_file_confidence: float = 0.0\n\nwith open('model.pth', 'rb') as f:\n    first_bytes = f.read(5)\n\nif first_bytes.raw_data[:3] == b'PK\\x03':\n    is_pickle_file_confidence = 0.5\n</code></pre> <p>In addition to this, the file extension will be checked. If it is <code>.pth</code>, confidence increases by 30%.</p> <pre><code>is_pickle_file_confidence: float\n\nif 'file.pth'.endswith('.pth'):\n    is_pickle_file_confidence += 0.3\n</code></pre> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and improve the analysis code.</p>"},{"location":"binarizers/#mlupmlbinarizationtorchtorchjitbinarizer","title":"mlup.ml.binarization.torch.TorchJITBinarizer","text":"<p>torch has several ways to save a model. JIT format is one of them. (You can read more in torch documentation).</p> <p>In fact, the code for loading the JIT model differs from the code for loading the <code>.pth</code> torch model only in calling the required function from the torch framework.</p> <pre><code>import torch\n\nwith open('file_path_jit.pth', 'rb') as f:\n    model = torch.jit.load(f)\nmodel.eval()\n</code></pre>"},{"location":"binarizers/#auto-search_4","title":"Auto search","text":"<p>This binarizer is not involved in the automatic selection of a binarizer, because we were unable to find distinctive features of this format.</p> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and add it.</p>"},{"location":"binarizers/#mlupmlbinarizationtensorflowtensorflowbinarizer","title":"mlup.ml.binarization.tensorflow.TensorFlowBinarizer","text":"<p>tensorflow has its own way of saving and loading the model. mlup uses exactly this. You can read more about ways to save and load your model in torch in documentation.</p> <p>The entire binarizer code can be reduced to approximately the following lines:</p> <pre><code>import tensorflow\n\nmodel = tensorflow.keras.models.load_model('model.keras')\n</code></pre>"},{"location":"binarizers/#auto-search_5","title":"Auto search","text":"<p>This binarizer is involved in automatic selection. Based solely on our observations, tensorflow models saved in the standard way have the same bytes at the beginning and end of the file. Similar to Pickle binarization.</p> <p>Considering that there is no official confirmation of this observation, this sign only adds 50% confidence. Also, the <code>mlup.ml.binarization.torch.TorchBinarizer</code> binarizer is based on the same feature and the same bytes.</p> <p>The following code does this check:</p> <pre><code>is_pickle_file_confidence: float = 0.0\n\nwith open('model.pth', 'rb') as f:\n    first_bytes = f.read(5)\n\nif first_bytes.raw_data[:3] == b'PK\\x03':\n    is_pickle_file_confidence = 0.5\n</code></pre> <p>In addition to this, the file extension will be checked. If it is <code>.keras</code> or <code>.h5</code>, confidence increases by 30%.</p> <pre><code>is_pickle_file_confidence: float\n\nif 'file.h5'.endswith('.keras') or 'file.h5'.endswith('.h5'):\n    is_pickle_file_confidence += 0.3\n</code></pre> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and improve the analysis code.</p>"},{"location":"binarizers/#mlupmlbinarizationtensorflowtensorflowsavedbinarizer","title":"mlup.ml.binarization.tensorflow.TensorFlowSavedBinarizer","text":"<p>tensorflow has several ways to save a model. Saved format is one of them. (You can read more in torch documentation).</p> <p>In fact, the code for loading the saved model differs from the code for loading the <code>.keras</code> tensorflow model only in calling the required function from the tensorflow framework.</p> <pre><code>import tensorflow\n\nmodel = tensorflow.saved_model.load('model.pb')\n</code></pre>"},{"location":"binarizers/#auto-search_6","title":"Auto search","text":"<p>In the automatic selection of a binarizer, this binarizer is based only on the <code>.pb</code> file extension. This sign adds only 30% confidence.</p> <pre><code>is_pickle_file_confidence: float = 0.0\n\nif 'file.pth'.endswith('.pb'):\n    is_pickle_file_confidence = 0.3\n</code></pre> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and improve the analysis code.</p>"},{"location":"binarizers/#mlupmlbinarizationonnxinferencesessionbinarizer","title":"mlup.ml.binarization.onnx.InferenceSessionBinarizer","text":"<p>Onnx is one of the most popular formats for saving models. It can be used to save models from different frameworks, such as using pickle.</p> <p>But the Python implementation has a different interface for using the loaded model than a simple call to the predict method with data passing. In the standard case, the onnx model predictor looks like this:</p> <pre><code>import onnxruntime\n\nmodel = onnxruntime.InferenceSession('model.onnx')\n\ninput_name = model.get_inputs()[0].name\npred = model.run(None, {input_name: [[1, 2, 3], [4, 5, 6]]})\n</code></pre> <p>Additional actions required: receiving inputs. To reduce the predictor to 1 action, mlup has its own wrapper for the onnx model:</p> <pre><code>import onnxruntime\n\n\nclass _InferenceSessionWithPredict(onnxruntime.InferenceSession):\n    def format_predict(self, predict_result):\n        if len(predict_result) &gt; 1:\n            return predict_result[:-1]\n        return predict_result\n\n    def predict(self, input_data):\n        input_name = self.get_inputs()[0].name\n        res = self.run(None, {input_name: input_data})\n        return self.format_predict(res)\n</code></pre> <p>The current mlup interface does not allow adding support for multiple inputs to the onnx model. This is due to the difference in incoming data. For the onnx model, for each input it is necessary to transmit data isolated for this input on all objects. And mlup takes all the data of one object together, and passes it to the model together.</p> <p>For example:</p> <pre><code>onnx_inputs = ['input1', 'input2', 'input3']\nobj1 = [1, 2, 3]\nobj2 = [4, 5, 6]\n# For onnx need data format\nfor_onnx_model = [{n: list(features)} for n, features in zip(onnx_inputs, zip(obj1, obj2))]\n\n# For mlup standart model\nfor_mlup_model = [obj1, obj2]\nprint(for_onnx_model)\nprint(for_mlup_model)\n</code></pre> <p>When working with a Python List, turning <code>for_mlup_model</code> into <code>for_onnx_model</code> is easy. But when the data arrives at the model, it has already been processed by the data transformer (See Data Transformers). Including a custom data transformer. Therefore, you cannot add generic code here to convert <code>for_mlup_model</code> to <code>for_onnx_model</code>. If you add code based on known mlup data transformers, then onnx models become limited to using these data transformers.</p> <p>Be careful! If you have multiple inputs, you can add these transformations to your first neural network layer.</p> <p>However, there can be any number of outputs from the model. They are all serialized and returned in the response mlup.</p> <p>Be careful! This binarizer always truncates the last element of the response if the response has more than 1 element.  This is because these models like to return all the classes and other side information on the predictor that the client may have in the last element.  If you don't need to trim the last element, use <code>mlup.ml.binarization.onnx.InferenceSessionFullReturnBinarizer</code>.</p> <p>The final code for loading onnx models is similar to this:</p> <pre><code>import onnxruntime\n\n\nclass _InferenceSessionWithPredict(onnxruntime.InferenceSession):\n    def format_predict(self, predict_result):\n        # Return model predict response in first items and all classes in last item\n        if len(predict_result) &gt; 1:\n            return predict_result[:-1]\n        return predict_result\n\n    def predict(self, input_data):\n        input_name = self.get_inputs()[0].name\n        res = self.run(None, {input_name: input_data})\n        return self.format_predict(res)\n\nmodel = _InferenceSessionWithPredict(\"/path/to/my/model.onnx\")\n</code></pre>"},{"location":"binarizers/#auto-search_7","title":"Auto search","text":"<p>This binarizer is involved in automatic selection. onnx has its own way of checking the correctness of the onnx file, which is what the binarizer uses. This sign brings 90% confidence.</p> <pre><code>import onnx\n\nis_pickle_file_confidence: float = 0.0\npath = 'model.onnx'\n\ntry:\n    onnx.checker.check_model(path)\n    is_pickle_file_confidence = 0.9\nexcept Exception:\n    pass\n</code></pre> <p>In addition to this, the file extension will be checked. If it is <code>.onnx</code>, confidence increases by 5%.</p> <pre><code>is_pickle_file_confidence: float\n\nif 'model.onnx'.endswith('.onnx'):\n    is_pickle_file_confidence += 0.05\n</code></pre> <p>Perhaps in the future we will conduct more in-depth research on binarization in this way and improve the analysis code.</p>"},{"location":"binarizers/#mlupmlbinarizationonnxinferencesessionfullreturnbinarizer","title":"mlup.ml.binarization.onnx.InferenceSessionFullReturnBinarizer","text":"<p>This binarization differs from <code>mlup.ml.binarization.onnx.InferenceSessionBinarizer</code> only in that it returns the complete response from the onnx model. Doesn't trim the last element.</p>"},{"location":"binarizers/#custom-binarizer","title":"Custom binarizer","text":"<p>If the capabilities of mlup binarizers are not enough for you, you can write your own binarizer.</p> <p>The binarizer interface is very simple:</p> <pre><code># my_module.py\nfrom typing import Any\nfrom mlup.constants import LoadedFile\nfrom mlup.ml.binarization.base import BaseBinarizer\n\n\nclass MyBinarizer(BaseBinarizer):\n    @classmethod\n    def deserialize(cls, data: LoadedFile) -&gt; Any:\n        pass\n</code></pre> <p>And specify the path to import your module in <code>binarization_type</code>: <code>my_module.MyBinarizer</code>.</p> <p>IMPORTANT: a binarizer written independently must be available for import on the server on which you run the mlup application.</p> <p>The easiest way to do this is to create your own python library with your binarizers and other useful classes and install it on your server along with the pymlup library.</p>"},{"location":"config_file/","title":"Config file","text":"<p>The mlup application configuration consists of two blocks: <code>ml</code>, <code>web</code>. When you configure using Python code, this separation does not exist. It is only in the config files.</p> <p>You don't always have to write a complete configuration file. You can explicitly specify only those parameters that differ from the default ones. All other parameters will be filled with default values.</p>"},{"location":"config_file/#what-settings-are-there","title":"What settings are there?","text":""},{"location":"config_file/#ml","title":"ml","text":""},{"location":"config_file/#information-arguments","title":"Information arguments","text":"<p><code>name (str)</code> - Model name. Use for information about your model.</p> <p>Default is \"MyFirstMLupModel\".</p> <p><code>version (str)</code> - Model version. Use for information about your model. </p> <p>Default is \"1.0.0.0\".</p>"},{"location":"config_file/#model-interface-settings","title":"Model interface settings","text":"<p><code>type (ModelLibraryType)</code> - Model type for choose current code for load and run model.     See mlup.constants.ModelLibraryType for details.</p> <p>Default is <code>ModelLibraryType.SKLEARN</code>.</p> <p><code>columns (Optional[List[Dict[str, str]]])</code> - Columns description for model predict method.     Format: List[Dict].</p> <pre><code>Example [{\"name\": \"col1\", \"type\": \"str\", \"required\": True, \"default\": None}]\nname - Column name;\ntype - Column type in string: int, str, float, bool.\nrequired - bool and optional field. By default is True.\nDefault - Any data and optional.\n</code></pre> <p>If you not set columns, then columns validation should be False. </p> <p><code>predict_method_name (str)</code> - Name for predict method by your model.     This param use for analyze predict method and run predict method in your model.</p> <p>Default is \"predict\".</p>"},{"location":"config_file/#model-load-settings","title":"Model load settings","text":"<p><code>auto_detect_predict_params (bool)</code> - If set is True, UP will parse predict method your model     and create from predict method arguments, validators and ApiScheme for inner data.     If set is False, then UP wait json with {data_for_predict: [..data for predict..]}.</p> <p>Default is True.</p> <p><code>storage_type (StorageType)</code> - Type storage, where should the model be loaded from.     All storages code there is in mlup.ml.storage package.     This param use in load() method, by it search storage class and use for load model.     Params for storage class, can set to storage_kwargs.     If set <code>StorageType.memory</code>, you can set ml to construct, without load from storage.</p> <p>You can write your storage class, for load model from you storage or with your logic.  For it, your storage class must be a class mlup.ml.storage.base.BaseStorage heir. See Storages for details.</p> <p>Default: StorageType.memory.</p> <p><code>storage_kwargs (Dict)</code> - Naming arguments for storage class from storage_type.</p> <p>Default: empty Dict.</p> <p><code>binarization_type (BinarizationType)</code> - Type binarization model method.     Use for load binary model from storage. See Binarizers for details.</p> <p>If set \"auto\", run auto search binarization type by model raw binary data and model name in storage. If binarization_type=\"auto\" found binarization, than set binarization_type to found binarizer class. If binarization_type=\"auto\" not found binarization, than raise mlup.errors.ModelLoadError.</p> <p>Default is \"auto\".</p>"},{"location":"config_file/#model-work-settings","title":"Model work settings","text":"<p><code>use_thread_loop (bool)</code> - Use concurrent.futures.ThreadPoolExecutor for create workers     and run in workers pool predict in model.     This useful for not blocking App CPU bound operations on the predict time.</p> <p>Default is True.</p> <p><code>max_thread_loop_workers (Optional[int])</code> - Count thread workers in <code>concurrent.futures.ThreadPoolExecutor</code>.     Param use if <code>use_thread_loop</code> is set True. If not set this param, then will be calculate,     how calculate concurrent library - min(32, os.cpu_count() + 4).</p> <p>Default is None, that is min(32, os.cpu_count() + 4).</p> <p><code>data_transformer_for_predict (ModelDataTransformerType)</code> - How data type need for model predict method.     See Data Transformers for details.</p> <p>Default ModelDataTransformerType.NUMPY_ARR.</p> <p><code>data_transformer_for_predicted (ModelDataTransformerType)</code> - How data type model returned from predict method.     See Data Transformers for details.</p> <p>Default ModelDataTransformerType.NUMPY_ARR.</p> <p><code>dtype_for_predict (Optional[str])</code> - Dtype for data_transformer_for_predict.     Each data_transformer uses its own way of searching for a dtype by the specified one.     But as a rule it is getattr([pandas, numpy, torch, tensorflow], dtype). See Data Transformers for details.</p> <p>Default is None, and the library itself determines which type.</p>"},{"location":"config_file/#web","title":"web","text":""},{"location":"config_file/#webapp-web-outer-interface-settings","title":"WebApp web outer interface settings","text":"<p><code>host (str)</code> - Web host for web app.</p> <p>Default is \"0.0.0.0\".</p> <p><code>port (int)</code> - Web port for web app.</p> <p>Default is 8009.</p> <p><code>web_app_version (str)</code> - User version for web_app.     Using the mlup version may not be nice to users your web app.     However, you can change the library version without changing the model.     And then the code of the web application backend actually changes.     You can use this field to notify users of these changes.</p> <p>Default is \"1.0.0.0\".</p> <p><code>column_validation (bool)</code> - Do I need to create a validator for columns from columns config     that are sent by the user through the web application?</p> <p>Default is False.</p> <p><code>custom_column_pydantic_model (Optional[Type[PydanticBaseModel]])</code> - Custom column pydantic model.     Use for validation. If set this field, flag column_validation not use. Validation will be always.</p> <p>Default is None.</p>"},{"location":"config_file/#webapp-architecture-settings","title":"WebApp architecture settings","text":"<p><code>mode (WebAppArchitecture)</code> - Work mode web app. We have three modes for work:     directly_to_predict, worker_and_queue, batching.      See Web app architectures for details.</p> <p>Default is WebAppArchitecture.directly_to_predict.</p> <p><code>max_queue_size (int)</code> - Max queue size with inner data for model.     It's use in modes worker_and_queue and batching, for controlling queue inner requests size.</p> <p>Default is 100.</p> <p><code>ttl_predicted_data (int)</code> - Max time member predict result data.     It's need for clean memory, if client not returned for predict result.</p> <p>Default is 60.</p> <p><code>ttl_client_wait (float)</code> - Max time wait results for clients, in single request.     If the client has not waited for a response within this time, the server responds with a 408 response code, and the user must make a second request.     This is necessary so as not to make customers wait forever and all the ensuing consequences.</p> <p>Default is 30.0</p> <p><code>min_batch_len (int)</code> - Min batch len for start predict.     Batching is regulated by two parameters: the batch accumulation time and the size of the accumulated batch.     If one of the parameters has reached its limit, then the batch predictor is launched.     It's min batch size for call batch predict.</p> <p>Default is 10.</p> <p><code>batch_worker_timeout (float)</code> - Max time for pending before run batching.     Batching is regulated by two parameters: the batch accumulation time and the size of the accumulated batch.     If one of the parameters has reached its limit, then the batch predictor is launched.     It's max time waiting batch for call batch predict.</p> <p>Default is 1.0.</p> <p><code>is_long_predict (bool)</code> - Added get-predict api method and add return predict_id to predict response.     Use only in worker_and_queue and batching modes.     If set this param, after request to /predict, client get predict_id in response.     If not set this param, then after <code>ttl_client_wait</code> time, client can't get predict result.</p> <p>Default is False.</p>"},{"location":"config_file/#webapp-work-settings","title":"WebApp work settings","text":"<p><code>show_docs (bool)</code> - Enable API docs in web app by URL /docs.</p> <p>Default is True</p> <p><code>debug (bool)</code> - Debug mode. Use for FastAPI and for show model configs.</p> <p>Default is False</p> <p><code>throttling_max_requests (Optional[int])</code> - Max count simultaneous requests to web app.     If set None, throttling by requests is disabling.</p> <p>Default is None</p> <p><code>throttling_max_request_len (Optional[int])</code> - Max count objects to predict in single request.     If set None, throttling by max items in request is disable.</p> <p>Default is None</p> <p><code>timeout_for_shutdown_daemon (float)</code>- Wait time for graceful shutdown web app.</p> <p>Default is 3.0</p> <p><code>uvicorn_kwargs (Dict)</code> - Uvicorn server kwargs arguments.</p> <p>Default is {}</p> <p><code>item_id_col_name (str)</code> - Column name for unique item_id.     Use in batching and worker. It need for marks requests in worker by request id.     Added this key for predicted data, but not thrown into model predict.</p> <p>Default is mlup.constants.ITEM_ID_COL_NAME - \"mlup_item_id\".</p>"},{"location":"config_file/#default-configuration-file","title":"Default configuration file","text":""},{"location":"config_file/#yaml","title":"yaml","text":"<pre><code>version: '1'\nml:\n  auto_detect_predict_params: true\n  binarization_type: auto\n  columns: null\n  data_transformer_for_predict: mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\n  data_transformer_for_predicted: mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\n  dtype_for_predict: null\n  max_thread_loop_workers: null\n  name: MyFirstMLupModel\n  predict_method_name: predict\n  storage_type: mlup.ml.storage.memory.MemoryStorage\n  type: sklearn\n  use_thread_loop: true\n  version: 1.0.0.0\nweb:\n  batch_worker_timeout: 1.0\n  column_validation: false\n  debug: false\n  host: 0.0.0.0\n  is_long_predict: false\n  item_id_col_name: mlup_item_id\n  max_queue_size: 100\n  min_batch_len: 10\n  mode: mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture\n  port: 8009\n  show_docs: true\n  throttling_max_request_len: null\n  throttling_max_requests: null\n  timeout_for_shutdown_daemon: 3.0\n  ttl_client_wait: 30.0\n  ttl_predicted_data: 60\n  uvicorn_kwargs: {}\n  web_app_version: 1.0.0.0\n</code></pre>"},{"location":"config_file/#json","title":"json","text":"<pre><code>{\n  \"version\": \"1\",\n  \"ml\": {\n    \"storage_type\": \"mlup.ml.storage.memory.MemoryStorage\", \n    \"data_transformer_for_predict\": \"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\", \n    \"type\": \"sklearn\", \n    \"data_transformer_for_predicted\": \"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\", \n    \"auto_detect_predict_params\": true, \n    \"version\": \"1.0.0.0\", \n    \"binarization_type\": \"auto\", \n    \"max_thread_loop_workers\": null, \n    \"dtype_for_predict\": null, \n    \"use_thread_loop\": true, \n    \"predict_method_name\": \"predict\", \n    \"columns\": null, \n    \"name\": \"MyFirstMLupModel\"\n  }, \n  \"web\": {\n    \"port\": 8009, \n    \"show_docs\": true, \n    \"ttl_client_wait\": 30.0, \n    \"timeout_for_shutdown_daemon\": 3.0, \n    \"uvicorn_kwargs\": {}, \n    \"batch_worker_timeout\": 1.0, \n    \"item_id_col_name\": \"mlup_item_id\", \n    \"throttling_max_request_len\": null, \n    \"is_long_predict\": false, \n    \"max_queue_size\": 100, \n    \"host\": \"0.0.0.0\", \n    \"min_batch_len\": 10, \n    \"debug\": false, \n    \"web_app_version\": \"1.0.0.0\", \n    \"mode\": \"mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture\", \n    \"throttling_max_requests\": null, \n    \"column_validation\": false, \n    \"ttl_predicted_data\": 60\n  }\n}\n</code></pre>"},{"location":"config_file/#how-to-get-the-default-configuration-file","title":"How to get the default configuration file","text":"<p>You can get the default configuration file in several ways, for example:</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\nmlup.generate_default_config('/path/to/your/config/file.yaml')\nmlup.generate_default_config('/path/to/your/config/file.json')\nup = mlup.UP(ml_model=EmptyModel())\nup.to_yaml('/path/to/your/config/file.yaml')\nup.to_json('/path/to/your/config/file.json')\ndict_config = up.to_dict()\n</code></pre>"},{"location":"config_file/#config-baselines","title":"Config baselines","text":"<p>In addition to the default config, mlup created configs for basic scenarios, changing the settings in them for a specific scenario. For example, for a tensorflow model or for a web app in batching mode.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Union, Optional\n\nfrom mlup.constants import ModelLibraryType, ModelDataTransformerType, BinarizationType, WebAppArchitecture\nfrom mlup.up import Config\n\n\n@dataclass\nclass WorkerAndQueueConfig(Config):\n    mode: WebAppArchitecture = WebAppArchitecture.worker_and_queue\n\n\n@dataclass\nclass BatchingConfig(Config):\n    mode: WebAppArchitecture = WebAppArchitecture.batching\n\n\n@dataclass\nclass ScikitLearnConfig(Config):\n    type: Union[str, ModelLibraryType] = ModelLibraryType.SCIKIT_LEARN\n    predict_method_name: str = 'predict'\n\n\n@dataclass\nclass ScikitLearnWorkerConfig(ScikitLearnConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.worker_and_queue\n\n\n@dataclass\nclass ScikitLearnBatchingConfig(ScikitLearnConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.batching\n\n\n@dataclass\nclass TensorflowConfig(Config):\n    type: Union[str, ModelLibraryType] = ModelLibraryType.TENSORFLOW\n    predict_method_name: str = '__call__'\n    data_transformer_for_predict: Union[str, ModelDataTransformerType] = ModelDataTransformerType.TENSORFLOW_TENSOR\n    data_transformer_for_predicted: Union[str, ModelDataTransformerType] = ModelDataTransformerType.TENSORFLOW_TENSOR\n    dtype_for_predict: Optional[str] = 'float32'\n\n\n@dataclass\nclass TensorflowWorkerConfig(TensorflowConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.worker_and_queue\n\n\n@dataclass\nclass TensorflowBatchingConfig(TensorflowConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.batching\n\n\n@dataclass\nclass TorchConfig(Config):\n    type: Union[str, ModelLibraryType] = ModelLibraryType.TORCH\n    predict_method_name: str = '__call__'\n    data_transformer_for_predict: Union[str, ModelDataTransformerType] = ModelDataTransformerType.TORCH_TENSOR\n    data_transformer_for_predicted: Union[str, ModelDataTransformerType] = ModelDataTransformerType.TORCH_TENSOR\n    dtype_for_predict: Optional[str] = 'float32'\n\n\n@dataclass\nclass TorchWorkerConfig(TorchConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.worker_and_queue\n\n\n@dataclass\nclass TorchBatchingConfig(TorchConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.batching\n\n\n@dataclass\nclass OnnxConfig(Config):\n    type: Union[str, ModelLibraryType] = ModelLibraryType.ONNX\n    binarization_type: Union[str, BinarizationType] = BinarizationType.ONNX_INFERENCE_SESSION\n\n\n@dataclass\nclass OnnxWorkerConfig(OnnxConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.worker_and_queue\n\n\n@dataclass\nclass OnnxBatchingConfig(OnnxConfig):\n    mode: WebAppArchitecture = WebAppArchitecture.batching\n</code></pre> <p>Using this config instead of the standard one is quite simple:</p> <pre><code>import mlup\nfrom mlup import baseline\nfrom mlup.ml.empty import EmptyModel\n\nup = mlup.UP(\n    ml_model=EmptyModel(), \n    conf=baseline.BatchingConfig(),\n)\n</code></pre>"},{"location":"data_transformers/","title":"Data transformers","text":"<p>As described in Description of the application life cycle, data transformers are needed to convert data from JSON format to model format and back. For example, from python list to numpy.array and back.</p> <p>mlup comes with several data transformers out of the box. This set corresponds to the supported binarization methods.</p> <ul> <li>mlup.ml.data_transformers.pandas_data_transformer.PandasDataFrameTransformer - docs is here.</li> <li>mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer - docs is here.</li> <li>mlup.ml.data_transformers.tf_tensor_data_transformer.TFTensorDataTransformer - docs is here.</li> <li>mlup.ml.data_transformers.torch_tensor_data_transformer.TorchTensorDataTransformer - docs is here.</li> <li>mlup.ml.data_transformers.src_data_transformer.SrcDataTransformer - docs in this document.</li> </ul> <p>These transformers can transform data from JSON format into their own format and back. There are two methods for this: <code>transform_to_model_format</code> and <code>transform_to_json_format</code>.</p>"},{"location":"data_transformers/#mlup-transformers","title":"mlup transformers","text":"<p>You can specify data transformers using the <code>data_transformer_for_predict</code> and <code>data_transformer_for_predicted</code> configuration parameters. Which are used to convert the data into model format and convert the response from the model accordingly.</p>"},{"location":"data_transformers/#mlupmldata_transformerspandas_data_transformerpandasdataframetransformer","title":"mlup.ml.data_transformers.pandas_data_transformer.PandasDataFrameTransformer","text":"<p>This transformer converts the incoming query into pandas tabular data. Since tabular data has column names, the column names and types are the data interface to the model's prediction. It is necessary to specify the columns, either in the configuration parameter columns or send a dictionary with columns directly in the request.</p> <pre><code>import mlup\nfrom mlup import constants\n\nclass Model:\n    def predict(self, X):\n        return X\n\ncolumns = [\n    {\"name\": \"col1\", \"type\": \"int\"},\n    {\"name\": \"col2\", \"type\": \"str\"},\n    {\"name\": \"col3\", \"type\": \"str\"},\n]\nobj1 = [1, 2, 3]\nobj2 = [4, 5, 6]\ndata_without_columns = [obj1, obj2]\ndata_with_columns = [\n    {c[\"name\"]: v for c, v in zip(columns, obj1)}, \n    {c[\"name\"]: v for c, v in zip(columns, obj2)}\n]\n\n# With columns in config\nup = mlup.UP(\n    ml_model=Model(),\n    conf=mlup.Config(\n        columns=columns,\n        data_transformer_for_predict=constants.ModelDataTransformerType.PANDAS_DF,\n        data_transformer_for_predicted=constants.ModelDataTransformerType.PANDAS_DF,\n    )\n)\nup.ml.load()\nprint(up.predict(X=data_without_columns))\n# [{'col1': 1, 'col2': 2, 'col3': 3}, {'col1': 4, 'col2': 5, 'col3': 6}]\n\nup.conf.columns = None\nup.ml.load(force_loading=True)\n\nprint(up.predict(X=data_with_columns))\n# [{'col1': 1, 'col2': 2, 'col3': 3}, {'col1': 4, 'col2': 5, 'col3': 6}]\n</code></pre> <p>For this data transformer, when creating pandas.DataFrame, you can specify dtype. To do this, use the <code>dtype_for_predict</code> configuration parameter. In this parameter, you can specify the Dtype name string in pandas. Here are some of them: <code>Float32Dtype</code>, <code>Float64Dtype</code>, <code>Int8Dtype</code>, <code>Int16Dtype</code>, <code>Int32Dtype</code>, <code>StringDtype</code>, <code>BooleanDtype</code>. If you do not specify the <code>dtype_for_predict</code> parameter in the configuration, then pandas.DataFrame determines the types itself.</p> <p>To reverse the conversion, call <code>pandas.DataFrame(...).to_dict(\"records\")</code>.</p>"},{"location":"data_transformers/#mlupmldata_transformersnumpy_data_transformernumpydatatransformer","title":"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer","text":"<p>This data transformer is used by default to transform data to and from model format.</p> <p>Regardless of the presence of columns in the query or config columns, after this transformer you will get <code>numpy.array</code>. If you have columns specified in the configuration or the data is sent with columns, they will affect the order of the resulting array. The data in the final array will be in the same order in which the columns are specified in the config or serialized by the web framework and turned into List[Dict].</p> <p>Therefore, it is not recommended to use this data transformer and send data through columns. But when specifying columns in the configuration, there will be no problems. Because the order of the columns is determined by the order in the config.</p> <pre><code>import mlup\nfrom mlup import constants\n\nclass Model:\n    def predict(self, X):\n        return X\n\ncolumns = [\n    {\"name\": \"col1\", \"type\": \"int\"},\n    {\"name\": \"col2\", \"type\": \"str\"},\n    {\"name\": \"col3\", \"type\": \"str\"},\n]\nobj1 = [1, 2, 3]\nobj2 = [4, 5, 6]\ndata_without_columns = [obj1, obj2]\ndata_with_columns = [\n    {c[\"name\"]: v for c, v in zip(columns[::-1], obj1[::-1])}, \n    {c[\"name\"]: v for c, v in zip(columns[::-1], obj2[::-1])}\n]\n\n# With columns in config\nup = mlup.UP(\n    ml_model=Model(),\n    conf=mlup.Config(\n        columns=columns,\n        data_transformer_for_predict=constants.ModelDataTransformerType.NUMPY_ARR,\n        data_transformer_for_predicted=constants.ModelDataTransformerType.NUMPY_ARR,\n    )\n)\nup.ml.load()\nprint(up.predict(X=data_with_columns))\n# [[1, 2, 3], [4, 5, 6]]\nprint(up.predict(X=data_without_columns))\n# [[1, 2, 3], [4, 5, 6]]\n\nup.conf.columns = None\nup.ml.load(force_loading=True)\n\nprint(up.predict(X=data_with_columns))\n# [[3, 2, 1], [6, 5, 4]]\nprint(up.predict(X=data_without_columns))\n# [[1, 2, 3], [4, 5, 6]]\n</code></pre> <p>For this data transformer, when creating numpy.array, you can specify dtype. To do this, use the <code>dtype_for_predict</code> configuration parameter. In this parameter, you can specify the dtype name string in numpy. Here are some of them: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>int16</code>, <code>bool_</code>.</p> <p>If you do not specify the <code>dtype_for_predict</code> parameter in the configuration, then numpy.array determines the types itself.</p> <p>To reverse the conversion, call <code>numpy.array(...).tolist()</code>.</p>"},{"location":"data_transformers/#mlupmldata_transformerstf_tensor_data_transformertftensordatatransformer","title":"mlup.ml.data_transformers.tf_tensor_data_transformer.TFTensorDataTransformer","text":"<p>This data transformer transforms data into tensorflow.Tensor, which is used in tensorflow models. The operating principle and algorithm are similar to <code>mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer</code>. Only the final conversion from <code>numpy.array</code> to <code>tensorflow.convert_to_tensor</code> is different.</p> <p>For this data transformer, when creating tensorflow.Tensor, you can specify dtype. To do this, use the <code>dtype_for_predict</code> configuration parameter. In this parameter, you can specify the dtype name string in tensorflow. Here are some of them: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>int16</code>, <code>bool</code>.</p> <p>If you do not specify the <code>dtype_for_predict</code> parameter in the configuration, then tensorflow.Tensor determines the types itself.</p> <p>To reverse the conversion, call <code>tensorflow.Tensor(...).numpy().tolist()</code>.</p>"},{"location":"data_transformers/#mlupmldata_transformerstorch_tensor_data_transformertorchtensordatatransformer","title":"mlup.ml.data_transformers.torch_tensor_data_transformer.TorchTensorDataTransformer","text":"<p>This data transformer transforms the data into torch.Tensor, which is used in torch models. The operating principle and algorithm are similar to <code>mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer</code>. Only the final conversion from <code>numpy.array</code> to <code>torch.tensor</code> is different.</p> <p>For this data transformer, when creating torch.Tensor, you can specify dtype. To do this, use the <code>dtype_for_predict</code> configuration parameter. In this parameter, you can specify the dtype name string in tensorflow. Here are some of them: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>int16</code>, <code>bool</code>.</p> <p>If you do not specify the <code>dtype_for_predict</code> parameter in the configuration, then torch.Tensor determines the types itself.</p> <p>To reverse the conversion, call <code>torch.Tensor(...).tolist()</code>.</p>"},{"location":"data_transformers/#mlupmldata_transformerssrc_data_transformersrcdatatransformer","title":"mlup.ml.data_transformers.src_data_transformer.SrcDataTransformer","text":"<p>This transformer does not do any conversions. There are scenarios when the model accepts python types or JSON valid data types. Also, the model can return python data or valid JSON data.</p> <p>For such scenarios this data transformer is used.</p>"},{"location":"data_transformers/#custom-data-transformer","title":"Custom data transformer","text":"<p>If the capabilities of mlup data transformers are not enough for you, you can write your own data transformer.</p> <p>The data transformer interface is very simple:</p> <pre><code># my_module.py\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, List, Union, Dict\nfrom mlup.ml.data_transformers.base import BaseDataTransformer\n\n\n@dataclass\nclass MyDataTransformer(BaseDataTransformer):\n    dtype_name: Optional[str] = None\n    dtype: Optional[Any] = None\n\n    def transform_to_model_format(\n        self,\n        data: List[Union[Dict[str, Any], List[Any]]],\n        columns: Optional[List[Dict[str, str]]] = None,\n    ):\n        pass\n\n    def transform_to_json_format(self, data: Any):\n        pass\n</code></pre> <p>And specify the path to import your module in <code>data_transformer_for_predict</code>: <code>my_module.MyDataTransformer</code> or <code>data_transformer_for_predicted</code>: <code>my_module.MyDataTransformer</code>.</p> <p>IMPORTANT: a data transformer written independently must be available for import on the server on which you run the mlup application.</p> <p>The easiest way to do this is to create your own python library with your data transformers and other useful classes and install it on your server along with the pymlup library.</p>"},{"location":"life_cycle/","title":"Life cycle","text":"<p>The entire life process of an mlup application can be divided into two stages - application initialization and running application.</p>"},{"location":"life_cycle/#initializing-the-application","title":"Initializing the application","text":"<p>mlup consists of two main components - ml and web.</p>"},{"location":"life_cycle/#mlupml","title":"mlup.ml","text":"<p>This component contains all the code for working with your model, including all settings associated with the model.</p> <p>When you create a <code>mlup.UP</code> object and pass a model <code>mlup.UP(ml_model=your_model)</code> into it, mlup does nothing. At this point, you have simply created a class. This is done in order to avoid starting the processes of loading and analyzing the model, which can be lengthy, without explicit instructions.</p> <p></p>"},{"location":"life_cycle/#upmlload","title":"up.ml.load()","text":"<p>To start using your model, you need to load the model into mlup - <code>up.ml.load()</code>. At this point, mlup first loads your model into memory. In the case of <code>mlup.UP(ml_model=your_model)</code> your model is already loaded and mlup will not load it into memory again.</p> <p>Loading a model into memory consists of two stages: * Loading binary data using <code>storage_type</code>; * Deserilization of binary data into the model;</p> <p>If you use <code>storage_type=mlup.constants.StorageType.disk</code>, by default binary data is not loaded into memory and binarizers themselves load the model during binarization. This was done to eliminate the possibility of duplicating model data in memory. Binaryizers have access to a local disk, so it is not difficult for them to load the model themselves. To change this behavior, <code>storage_type=mlup.constants.StorageType.disk</code> has a <code>need_load_file</code> flag, which defaults to False.</p> <p>The storage concept is needed to load binary model data from different storages to a local disk, from where binarizers will read the data.</p> <p>Once the model is loaded in memory, <code>up.ml.load_model_settings()</code> is called inside <code>up.ml.load()</code>.</p> <p>You can check whether the model has been loaded into memory using the <code>up.ml.loaded: bool</code> property. It becomes True only when the model is in memory.  But it does not indicate whether the model was analyzed using the <code>up.ml.load_model_settings()</code> method.</p>"},{"location":"life_cycle/#upmlload_model_settings","title":"up.ml.load_model_settings()","text":"<p>The <code>up.ml.load_model_settings()</code> method analyzes the loaded model, according to the settings specified in the config, and prepares the <code>mlup.UP</code> object to work with it.</p> <p>At this point the analysis occurs: * Method for prediction; * Method arguments for prediction; * Creating data transformers to convert user data into model format and model response into user format; * Creation of auxiliary entities, such as <code>concurrent.futures.ThreadPoolExecutor</code>;</p> <p>After this, your model is ready to be used via mlup.</p> <p>If you change some setting related to the operation of the model, just call <code>up.load_model_settings()</code>. Then mlup will not reload your model into memory, but will simply analyze it again taking into account the changed config.</p> <pre><code>import numpy\nimport mlup\n\nclass MyModel:\n    def predict(self, X):\n        return X\n\nmodel = MyModel()\n\nup = mlup.UP(ml_model=model, conf=mlup.Config(auto_detect_predict_params=True))\nup.ml.load()\n\nobj_1 = [1, 2, 3]\nobj_2 = [4, 5, 6]\nobjs_for_predict = [obj_1, obj_2]\nup.predict_from(X=numpy.array(objs_for_predict))\n\nup.conf.auto_detect_predict_params = False\n# Refresh mlup model settings\nup.ml.load_model_settings()\n\nup.predict_from(data_for_predict=numpy.array(objs_for_predict))\n</code></pre>"},{"location":"life_cycle/#mlupweb","title":"mlup.web","text":"<p>To launch a web application, it also needs to be initialized with <code>up.web.load()</code>.</p> <p>When using <code>up.run_web_app()</code>, you don't need to worry about this - mlup will initialize it itself. But if the web component has already been initialized, you need to specify the <code>up.run_web_app(force_load=True)</code> parameter. If you want to launch your web application in any of the other ways, you need to take care of initializing it yourself - call the <code>up.web.load()</code> method.</p> <p>P.S. You don't have to worry if you forget to call <code>up.web.load()</code>. mlup will not be able to launch the web application and will crash with the error WebAppLoadError</p> <p>Unlike the ml component, the web component has only 1 loading method, <code>up.web.load()</code>, which each time recreates the web app application and reinitializes all settings.</p> <p>Loading a web application consists of several sequential stages (all of them are hidden inside <code>up.web.load()</code>): * <code>up.web.load_web_app_settings()</code> - preloading parameters for initializing and launching the application. Here transformations and filling of internal structures take place for the configuration of the web application and the backend of the application. * <code>up.web._create_app()</code> - initialization of the web application itself and its backend, construction of the API and application documentation. This is where the analysis results from your machine learning model are most used. * <code>self._architecture_obj.load()</code> - initialization of the web application architecture. Some architectures require some operations before launch: initializing workers and queues, adding custom API methods to a web application, and other operations. (See Web app architectures).</p> <p>Almost always, you need to call <code>up.web.load()</code> and not worry about its internals. But understanding the initialization order will help you write your own modules correctly and customize the application.</p> <p>Because the web component builds the API and validation for incoming data, it uses the results of your model analysis. This means that you will not be able to initialize web first and then ml. The web component needs an initialized ml component to initialize.</p> <p>Just like in the case of ml, web has the <code>up.web.loaded</code> attribute. It becomes True if and only if a <code>fastapi.FastAPI</code> application is created.  Those only after calling <code>up.web.load()</code>.</p> <p>If your application has already called <code>up.web.load()</code>, and then you change the settings, this attribute will still be True.  You need to independently monitor the reinitialization of the application after updating the config.</p> <pre><code>import mlup\n\nclass MyModel:\n    def predict(self, X):\n        return X\n\nmodel = MyModel()\n\nup = mlup.UP(ml_model=model)\nup.ml.load()\n\nobj_1 = [1, 2, 3]\nobj_2 = [4, 5, 6]\nobjs_for_predict = [obj_1, obj_2]\nmodel_predicted = model.predict(X=objs_for_predict)\nmlup_predicted = up.predict(X=objs_for_predict)\n\nprint(f'Before call up.web.load: {up.web.loaded}')\nup.web.load()\nprint(f'After call up.web.load and before change config: {up.web.loaded}')\nup.conf.port = 8011\nprint(f'After call up.web.load and after change config: {up.web.loaded}')\nup.web.load()\nprint(f'After double call up.web.load: {up.web.loaded}')\nup.run_web_app()\n\nimport requests\nresp = requests.post('http://0.0.0.0:8011/predict', json={'X': objs_for_predict})\nweb_app_predicted = resp.json()\n\nup.stop_web_app()\n\nprint(model_predicted)\nprint(mlup_predicted)\nprint(web_app_predicted)\n</code></pre> <p>After successful initialization of the web component, you can launch your application in a way convenient for you.</p>"},{"location":"life_cycle/#ml-predict-process","title":"Ml predict process","text":"<p>The <code>up.ml.predict</code> method calls the web application for prediction. But you also use it when you call any of the methods: <code>up.predict</code>, <code>up.predict_from</code>, <code>up.async_predict</code>.</p> <p>Behind it lies a process of several stages: * First of all, the main array with object attributes - X - is searched in the transmitted user data. To do this, the <code>up.ml.get_X_from_predict_data</code> method is called. * If X is found, in other words the data is not empty, then the found data X is passed through the data transformer specified in the <code>data_transformer_for_predict</code> configuration parameter. Called <code>up.ml._transform_data_for_predict</code>. See Data Transformers. * If there are problems with the transformation, a PredictTransformDataError exception will be thrown. If the transformation is successful, the data is used for prediction. * Prediction is the actual call to <code>predict</code> on the model with the transformed data. But depending on the configuration and the (use_thread_loop) Description of the configuration file parameter, the prediction is run with or without <code>concurrent.futures.ThreadPoolExecutor</code>. * If the prediction caused an error, a PredictError exception will be thrown. * And if the prediction is successful, the prediction results will be sent to the data transformer from the <code>data_transformer_for_predicted</code> configuration parameter for conversion to a valid JSON format and returned from the method. Called <code>up.ml._transform_predicted_data</code>.</p> <p></p>"},{"location":"life_cycle/#web-application-work","title":"Web application work","text":"<p>After launching the web application, it is ready to accept requests from users. When a developer independently writes a web application with a model, he has full control and knowledge of how the request processing process occurs. mlup takes care of this, so the entire request processing process is described here.</p> <p></p> <p>The process looks like this: * A request has been received from a user. * If this is not a /predict request, then it immediately goes to the handler. If <code>/predict</code>, wrappers for the handler are processed first:   * <code>mlup.web.app._set_predict_id_to_response_headers</code> - generates predict_id and sets the request headers to it. Thus, it can be used in the request handler itself and its subfunctions.   * <code>mlup.web.app._requests_throttling</code> - throttling by the number of requests to the predictor. If the request is not throttled, the web application responds with a 429 response code. Enabling/disabling throttling and configuration can be found in Description of the configuration file (throttling_max_requests). * If not <code>/predict</code>, a JSON response is generated and returned to the client. Next we'll look at the <code>/predict</code> method. * During web application initialization, a pydantic model is created to validate incoming data. The Pydantic model is based on the result of model analysis and mlup configuration and is available in the <code>up.web._predict_inner_pydantic_model</code> attribute. Incoming data is run and validated through this pydantic model. If there are errors, the application immediately responds with a 422 response code with validation errors. * If the validation is successful, validation occurs for the length of the request - the number of objects for predict in the request. If the request is not throttled, the web application responds with a 429 response code. Enabling/disabling and configuration of request length throttling can be found in Description of the configuration file (throttling_max_request_len) * After successfully passing through all stages of validation and throttling, the request is transferred to the architecture for processing, to the <code>up.web._architecture_obj.predict</code> method. * After receiving a response from the architecture component, the web application responds with what the architecture component responded.</p> <p>The processing code itself is short:</p> <pre><code>@_set_predict_id_to_response_headers\n@_requests_throttling\nasync def predict(self, request: FastAPIRequest, response: FastAPIResponse):\n    predict_id = response.headers[PREDICT_ID_HEADER]\n    # Validation\n    try:\n        predict_request_body = self._predict_inner_pydantic_model(**(await request.json()))\n    except json.JSONDecodeError as e:\n        raise PredictValidationInnerDataError(msg=f'Invalid json data: {e}', predict_id=predict_id)\n    data_for_predict = predict_request_body.dict()\n\n    # Throttling\n    if self.conf.throttling_max_request_len:\n        self._request_len_throttling(data_for_predict)\n\n    # Predict in web app architecture object\n    predict_result = await self._architecture_obj.predict(data_for_predict, predict_id=predict_id)\n\n    # Result\n    return {\"predict_result\": predict_result}\n</code></pre>"},{"location":"life_cycle/#web-application-customization","title":"Web application customization","text":"<p>If you need to add your own code to initialize the FastAPI application, it is available through the <code>up.web.app: fastapi.FastAPI</code> attribute.</p> <p>The web application becomes available only after calling <code>up.web.load()</code>. In this method it's created.</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\n\nup = mlup.UP(ml_model=EmptyModel(), conf=mlup.Config())\nup.ml.load()\nup.web.load()\n\napp = up.web.app\n\n\ndef my_api_method():\n    return {}\n\n\napp.add_api_route(\"/my-api-method\", my_api_method, methods=[\"GET\"], name=\"my-api-method\")\nup.run_web_app()\n</code></pre>"},{"location":"python_interface/","title":"Description of the Python Interface","text":"<p>You can use all the features of mlup directly in python code.</p> <p>To do this, you need to import mlup into your environment.</p> <pre><code>import mlup\n</code></pre> <p>After that, you can create as many <code>mlup.UP</code> applications as you like.</p> <pre><code>import mlup\n\nup1 = mlup.UP()\nup2 = mlup.UP()\n# ...\n</code></pre>"},{"location":"python_interface/#mlupup","title":"mlup.UP","text":"<p>The <code>mlup.UP</code> object can be created in different ways: * Create models in the variable <code>mlup.UP(ml_model=your_model)</code> * Create from the config <code>mlup.UP.load_from_yaml(path_ti_your_conf)</code> * Create with empty model:</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\nup = mlup.UP(ml_model=EmptyModel())\n</code></pre> <p>You can also specify your own config when creating:</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\nup = mlup.UP(\n    ml_model=EmptyModel(),\n    conf=mlup.Config(\n        # ...\n    )\n)\n</code></pre> <p><code>mlup.UP</code> has methods: * <code>load_from_dict</code> - Creating a <code>mlup.UP</code> object from a dictionary with a config. * <code>load_from_yaml</code> - Creating a <code>mlup.UP</code> object from a file with yaml config. * <code>load_from_json</code> - Creating a <code>mlup.UP</code> object from a file with a json config. * <code>to_dict</code> - Returns the config of an existing <code>mlup.UP</code> in the form of a dictionary. * <code>to_yaml</code> - Saves the existing <code>mlup.UP</code> config to a yaml file. * <code>to_json</code> - Saves the existing <code>mlup.UP</code> config to a json file. * <code>predict</code> - Calls model prediction <code>mlup.UP.ml.predict</code> on the running event_loop. * <code>predict_from</code> - Same as <code>predict</code>, but without data processing before prediction. * <code>async async_predict</code> - Asynchronous version of <code>predict</code>. Fires in your event_loop. * <code>run_web_app</code> - Runs a web application. * <code>stop_web_app</code> - Stops a running web application.</p> <p>And also, there are properties: * <code>mlup.UP.ml</code> is a wrapper over the ml model. It contains all the logic for working with the model and processing data for it. * <code>mlup.UP.web</code> is a wrapper for the web application. This object contains all the logic for creating, configuring and operating a web application.</p> <p>You can change the configuration while <code>mlup.UP</code> is alive, without having to recreate the <code>mlup.UP</code> object.</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\nup = mlup.UP(\n    ml_model=EmptyModel(),\n    conf=mlup.Config(\n        # ...\n    )\n)\nup.ml.load()\n\nup.conf.use_thread_loop = False\nup.ml.load(force_loading=True)\n</code></pre>"},{"location":"python_interface/#mlupupml","title":"mlup.UP.ml","text":"<p>The <code>mlup.UP.ml</code> object is <code>mlup.ml.model.MLupModel</code>.</p> <p><code>mlup.UP.ml</code> is a wrapper around your ml model. Other than the <code>mlup.UP.ml.load</code> method, in most cases you won't need to access it directly.</p> <p><code>mlup.UP.ml</code> has methods: * <code>load</code> - a method that loads the model into memory, analyzes it and prepares the <code>MLupModel</code> object for working with the model. * <code>load_model_settings</code> - only parses the loaded model and configures the internals of <code>MLupModel</code> after parsing. Called inside <code>load</code>. * <code>get_X_from_predict_data</code> - searches and extracts from the data for prediction, the main argument with features X, according to the results of the analysis of the loaded model. * <code>async predict</code> - causes model prediction, along with data processing. * <code>async predict_from</code> - the same as <code>predict</code>, but without processing before prediction.</p> <p>You can read about scenarios for using the <code>load</code>, <code>load_model_settings</code>, <code>get_X_from_predict_data</code> methods in Description of the application life cycle.</p> <p>The <code>predict</code> and <code>predict_from</code> methods accept data for prediction based on the keys for which you send it to the model.</p> <pre><code>import numpy\nimport mlup\nfrom sklearn.tree import DecisionTreeClassifier\n\nx = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [2, 3, 4], [3, 4, 5], [5, 6, 7]])\ny = numpy.array([1, 0, 1, 1, 0, 1])\nmodel = DecisionTreeClassifier()\nmodel.fit(x, y)\n# Sklearn model have X, how main argument for features\nprint(\"Predict directly from ml model\")\nprint(model.predict(X=x))\n\nup = mlup.UP(ml_model=model)\nup.ml.load()\nprint(\"Predict from mlup.UP\")\nprint(up.predict(X=x.tolist()))\nprint(\"Predict from mlup.UP from numpy\")\nprint(up.predict_from(X=x))\n</code></pre> <p>P.S. The web application uses the <code>predict</code> method to call a prediction for the client.</p> <p>You can also check whether the model has been loaded and parsed by using <code>loaded</code>.</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\n\nup = mlup.UP(ml_model=EmptyModel())\nprint(\"Before mlup.UP.ml.load()\")\nprint(up.ml.loaded)\n\nup.ml.load()\nprint(\"After mlup.UP.ml.load()\")\nprint(up.ml.loaded)\n\nup.conf.use_thread_loop = False\nprint(\"After mlup.UP.ml.load() and change model config\")\nprint(up.ml.loaded)\n</code></pre> <p>If you need to access your model, you can do so through the <code>model_obj</code> property.</p> <pre><code>import mlup\nfrom mlup.errors import ModelLoadError\nfrom mlup.ml.empty import EmptyModel\n\nmodel = EmptyModel()\nup = mlup.UP(ml_model=model)\nprint(\"Before mlup.UP.ml.load()\")\ntry:\n    m = up.ml.model_obj\nexcept ModelLoadError:\n    print(\"Model not loaded for get source model\")\n\nup.ml.load()\nprint(\"After mlup.UP.ml.load()\")\nprint(up.ml.model_obj)\nprint(model is up.ml.model_obj)\n</code></pre>"},{"location":"python_interface/#mlupupweb","title":"mlup.UP.web","text":"<p>The <code>mlup.UP.web</code> object is <code>mlup.web.app.MLupWebApp</code>.</p> <p><code>mlup.UP.web</code> is a wrapper around a FastAPI web application. In most cases, you won't need to contact it directly.</p> <p><code>mlup.UP.web</code> has methods: * <code>load</code> - a method that creates a web application, according to the config and the results of the analysis of <code>mlup.ml.model.MLupModel.load()</code>. * <code>load_web_app_settings</code> - prepares some internal configs for launching the web application. Called inside <code>load</code>. * <code>run</code> - Launches the web application. * <code>stop</code> - Stops a running web application. * <code>async http_health</code> - This is a handler for the <code>/health</code> request. * <code>async info</code> - This is the <code>/info</code> request handler in case of <code>debug=False</code>. * <code>async debug_info</code> - This is the <code>/info</code> request handler in the case of <code>debug=True</code>. * <code>async http_health</code> - This is the <code>/predict</code> request handler.</p> <p>You can read about scenarios for using the <code>load</code>, <code>load_web_app_settings</code>, <code>run</code>, <code>stop</code> methods in Description of the application life cycle.</p> <pre><code>import numpy\nimport mlup\nimport requests\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nx = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [2, 3, 4], [3, 4, 5], [5, 6, 7]])\ny = numpy.array([1, 0, 1, 1, 0, 1])\nmodel = DecisionTreeClassifier()\nmodel.fit(x, y)\n# Sklearn model have X, how main argument for features\nprint(\"Predict directly from ml model\")\nprint(model.predict(X=x))\n\nup = mlup.UP(ml_model=model)\nup.ml.load()\nprint(\"Predict from mlup.UP\")\nprint(up.predict(X=x.tolist()))\n\n# mlup.UP.web.load calling inner up.run_web_app method\nup.run_web_app(daemon=True)\ntime.sleep(1)\nresp = requests.post(\"http://0.0.0.0:8009/predict\", json={\"X\": x.tolist()})\n\nprint(\"Predict from web app\")\nprint(resp.json())\nup.stop_web_app()\n</code></pre> <p>You can also check if the web application has been created and configured by using <code>loaded</code>.</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\n\nup = mlup.UP(ml_model=EmptyModel())\nprint(\"Before mlup.UP.web.load()\")\nprint(up.web.loaded)\n\nup.ml.load()\nprint(\"After mlup.UP.ml.load()\")\nprint(up.web.loaded)\n\nup.web.load()\nprint(\"After mlup.UP.web.load()\")\nprint(up.web.loaded)\n\nup.conf.port = 8010\nprint(\"After mlup.UP.web.load() and change web app config\")\nprint(up.web.loaded)\n</code></pre> <p>If you need to access a FastAPI-generated web application, you can do so through the <code>app</code> property.</p> <pre><code>import mlup\nfrom mlup.errors import WebAppLoadError\nfrom mlup.ml.empty import EmptyModel\nfrom fastapi import FastAPI\n\nmodel = EmptyModel()\nup = mlup.UP(ml_model=model)\nup.ml.load()\nprint(\"Before mlup.UP.web.load()\")\ntry:\n    w = up.web.app\nexcept WebAppLoadError:\n    print(\"Web app not loaded for get created web app\")\n\nup.web.load()\nprint(\"After mlup.UP.web.load()\")\nprint(up.web.app)\nprint(isinstance(up.web.app, FastAPI))\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#fast-run","title":"Fast run","text":"<p>The easiest way to try to launch a web application with basic settings is to call the bash command:</p> <pre><code>mlup run -m /path/to/your/model.pckl\n</code></pre> <p>or you can do it directly in jupyter notebook, if your model is still in a variable:</p> <pre><code>import mlup\n\nmodel: YourModel\n\nup = mlup.UP(ml_model=model)\nup.ml.load()\nup.run_web_app(daemon=True)\n# Testing your web application\nup.stop_web_app()\n</code></pre> <p>After launching the web application, you can check how it works. Open http://0.0.0.0:8009/docs to view the documentation of your API. There, you can immediately try sending a request to <code>/predict</code>.</p> <p>You can pass your settings directly to the bash command:</p> <pre><code>mlup run -m /path/to/your/model.pckl --up.port=8011\n</code></pre> <p>or in your code:</p> <pre><code>import mlup\n\nmodel: YourModel\n\nup = mlup.UP(ml_model=model, conf=mlup.Config(port=8011))\nup.ml.load()\nup.run_web_app(daemon=True)\n# Testing your web application\nup.stop_web_app()\n</code></pre> <p>You can read about all the settings at description of the configuration file.</p> <p>You can check how the data is processed, the model makes a prediction, and the response is processed without launching the web application. There are methods for this: * <code>UP.predict</code> - Method that is called by the web application with the data received in the request. If the <code>auto_detect_predict_params=True</code> flag is set in the config (See Config: auto_detect_predict_params), the arguments of this method are the same as the arguments of the model's predict method. If the <code>auto_detect_predict_params=False</code> flag is set in the config, the data is passed in the <code>data_for_predict</code> argument. * <code>UP.async_predict</code> - Asynchronous version of <code>UP.predict</code>. * <code>UP.predict_from</code> - Same as <code>UP.predict</code>, but does not call the data transformer before calling the model predictor. This allows you to quickly test the model, without transforming your test data into a valid JSON format.</p> <pre><code>import numpy\nimport mlup\n\nclass MyModel:\n    def predict(self, X):\n        return X\n\nmodel = MyModel()\n\nup = mlup.UP(ml_model=model, conf=mlup.Config(auto_detect_predict_params=True))\nup.ml.load()\n\nobj_1 = [1, 2, 3]\nobj_2 = [4, 5, 6]\nobjs_for_predict = [obj_1, obj_2]\nup.predict(X=objs_for_predict)\nawait up.async_predict(X=objs_for_predict)\nup.predict_from(X=numpy.array(objs_for_predict))\n\nup.conf.auto_detect_predict_params = False\n# Refresh mlup model settings\nup.ml.load_model_settings()\n\nup.predict(data_for_predict=objs_for_predict)\nawait up.async_predict(data_for_predict=objs_for_predict)\nup.predict_from(data_for_predict=numpy.array(objs_for_predict))\n</code></pre>"},{"location":"quickstart/#different-model-types","title":"Different model types","text":"<p>By default, mlup calls the model's <code>predict</code> method. This behavior can be changed using the <code>predict_method_name=\"predict\"</code> parameter. For models that are callable, <code>predict_method_name=\"__call__\"</code> should be specified. For example, for <code>tensorflow</code>, <code>torch</code> models.</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\nup = mlup.UP(ml_model=EmptyModel(), conf=mlup.Config(predict_method_name=\"__call__\"))\n</code></pre> <p>Also, models can be binarized in different ways: <code>pickle</code>, <code>joblib</code>, <code>onnx</code>, etc. By default, mlup tries the pickle binarizer (mlup.ml.binarization.pickle.PickleBinarizer). This behavior can be changed by specifying the <code>binarization_type</code> parameter. You can specify one of the mlup binarizers or specify your own (See Binarizers).</p>"},{"location":"quickstart/#launch-on-servers","title":"Launch on servers","text":"<p>There is one important difference between the local configuration and the server configuration. On the server, the model is always loaded from storage - for example, from a local disk. On the local, you can load the model directly from a variable.</p> <p>P.S. When you pickle <code>mlup.UP</code> of an object, the model is saved along with the <code>mlup.UP</code> object and is not additionally loaded from disk.</p> <p>To do this, you need to specify the path to the model on the server in the config. Two parameters are responsible for this: <code>storage_type</code> and <code>storage_kwargs</code>.</p> <pre><code>import mlup\nfrom mlup import constants\n\nup = mlup.UP(\n    conf=mlup.Config(\n        storage_type=constants.StorageType.disk,\n        storage_kwargs={\n            'path_to_files': '/path/to/your/model/on/server.extension',\n            'files_mask': 'server.extension',\n        },\n    )\n)\n</code></pre> <p>mlup creates an object from <code>storage_type</code> and uses <code>storage_kwargs</code> as creation arguments. In the case of <code>mlup.constants.StorageType.disk</code>, you must specify the path to the <code>path_to_files</code> model and can specify <code>file_mask</code>. <code>file_mask</code> is a regular expression that will find your model in <code>path_to_files</code>.</p> <p>By default, mlup <code>storage_type=constants.StorageType.memory</code>.</p>"},{"location":"storages/","title":"Storages","text":"<p>Description of the application life cycle describes the role of storage throughout mlup. This component is needed to deliver the model from your storage to the local disk and transfer information about the downloaded file to the binarizer (See Binarizers). It can also load the contents of files into memory.</p> <p>This can be any storage that your code can access from the server using any protocol.</p> <p>At the moment there are no implementations for working with remote storages in mlup \ud83d\ude43</p> <p>So far, mlup supports two types of storage out of the box: * mlup.ml.storage.memory.MemoryStorage * mlup.ml.storage.local_disk.DiskStorage</p> <p>You can select the desired storage using the <code>storage_type</code> configuration parameter.</p> <p>The storage class may need additional parameters that govern its operation. For example, login and password for storage. To pass these parameters to the storage class, you need to use the <code>storage_kwargs: Dict[str, Any]</code> configuration parameter. Parameters from <code>storage_kwargs</code> are passed to the storage class constructor: <code>storage_class(**storage_kwargs)</code>.</p>"},{"location":"storages/#mlupmlstoragememorymemorystorage","title":"mlup.ml.storage.memory.MemoryStorage","text":"<p>This type is used when your model is already loaded into memory and does not need to be loaded and binarized. It is the default in the mlup configuration.</p> <p>For example:</p> <pre><code>import mlup\nfrom mlup.ml.empty import EmptyModel\n\n\nup = mlup.UP(ml_model=EmptyModel())\n</code></pre> <p>The example above uses <code>storage_type=mlup.ml.storage.memory.MemoryStorage</code>.</p> <p>This storage has no parameters for <code>storage_kwargs</code>.</p>"},{"location":"storages/#mlupmlstoragelocal_diskdiskstorage","title":"mlup.ml.storage.local_disk.DiskStorage","text":"<p>When you run a mlup application on the server, at that moment your model is not loaded into memory and needs to be loaded.</p> <p><code>mlup.ml.storage.local_disk.DiskStorage</code> finds the model file by path and mask on the local disk, and returns information on the found file, which mlup passes to the binarizer.</p> <p>This storage has parameters for <code>storage_kwargs</code>: * <code>path_to_files: str</code> - Required. The path to the folder with the model file or to the model file itself on disk. * <code>file_mask: str</code> = Optional. By default <code>(\\w.-_)*.pckl</code> is pickle. A regular expression that will be used to search for a file in <code>path_to_files</code>. * <code>need_load_file: bool</code> - Optional. Default is False. If False, the binary model data itself will not be loaded into memory and the binarizer will load it itself. If True, then the storage will load the raw data into memory and give it to the binarizer.</p> <p>IMPORTANT! * By default, <code>mlup.ml.storage.local_disk.DiskStorage</code> does not load model data into memory. The binarizer loads them independently. In this case, if your model weighs 1 GB, then you need at least 1 GB of RAM to run the application. * If you specify <code>need_load_file=True</code>, and the model is loaded into memory by storage, memory duplication will occur: raw data loaded by storage and the model serialized from this data. It turns out that the increase in memory consumed by the application for launching doubles the weight of the model. If your model weighs 1 GB, then you will need at least 2 GB to run the application. But after running, the memory consumption should drop back to 1 GB because mlup explicitly deletes the raw data after serialization and calls the garbage collector.</p> <pre><code>import gc\nfrom mlup.constants import LoadedFile\n\nloaded_files: list[LoadedFile] = storage.load()\n# ...Binaraizer code...\ndel loaded_files\ngc.collect()\n</code></pre> <p>An example of using storage:</p> <pre><code>import mlup\nfrom mlup import constants\n\nup = mlup.UP(\n    conf=mlup.Config(\n        storage_type=constants.StorageType.disk,\n        storage_kwargs={\n            \"path_to_files\": \"/path/to/my/model.onnx\",\n            # There may be several files in the folder. \n            # To uniquely identify your model file, you can use its full name in the mask.\n            \"file_mask\": \"model.onnx\",\n        },\n    )\n)\nup.ml.load()\n</code></pre>"},{"location":"storages/#custom-storage","title":"Custom storage","text":"<p>If the capabilities of mlup storages are not enough for you, you can write your own storage.</p> <p>The storage interface is very simple:</p> <pre><code># my_module.py\nfrom dataclasses import dataclass\nfrom typing import List, Union\nfrom mlup.constants import LoadedFile\nfrom mlup.ml.storage.base import BaseStorage\n\n\n@dataclass\nclass MyStorage(BaseStorage):\n    password: str\n    login: str = 'default login'\n\n    @classmethod\n    def load_bytes_single_file(cls, *args, **kwargs) -&gt; Union[str, bytes]:\n        pass\n\n    @classmethod\n    def load(cls) -&gt; List[LoadedFile]:\n        pass\n</code></pre> <p>Where: * <code>load</code> - method that calls mlup. Everything happens in this method. It also calls <code>load_bytes_single_file</code> for each file that needs to be analyzed, loaded into memory or downloaded to disk. * <code>load_bytes_single_file</code> - this method is called inside <code>load</code> for each file when the file needs to be parsed, loaded into memory or downloaded to its disk.</p> <p>And specify the path to import your module in <code>storage_type=\"my_module.MyStorage\"</code>. And <code>storage_kwargs={\"password\": \"password from my storage\", \"login\": \"not default login\"}</code>.</p> <p>IMPORTANT: a storage written independently must be available for import on the server on which you run the mlup application.</p> <p>The easiest way to do this is to create your own python library with your storages and other useful classes and install it on your server along with the pymlup library.</p>"},{"location":"web_app_api/","title":"Web app API","text":"<p>The web application has an API of several methods:</p> <ul> <li>[HEAD, OPTIONS, GET] /health - Web application status. Must answer with code 200.</li> <li>[GET] /info - Information about the model, application, and its settings.</li> <li>[POST] /predict - call model prediction.</li> <li>[GET] /get-predict/{predict_id} - optional point. Obtaining prediction results.</li> </ul>"},{"location":"web_app_api/#api","title":"API","text":""},{"location":"web_app_api/#health","title":"/health","text":"<p>This is a simple web application state API method. Allows monitoring to ensure that the application is not frozen and can process requests.</p>"},{"location":"web_app_api/#head","title":"HEAD","text":"<ul> <li>status_code: 200</li> </ul>"},{"location":"web_app_api/#get-options","title":"GET, OPTIONS","text":"<ul> <li>status_code: 200</li> <li>body: <code>{\"status_code\": 200}</code></li> </ul>"},{"location":"web_app_api/#info","title":"/info","text":"<p>This method returns information about the application and model, which allows the model client to know which version of the model and application it is working with, as well as some additional settings.</p>"},{"location":"web_app_api/#get","title":"GET","text":"<ul> <li>status_code: 200</li> </ul> <p>If <code>debug=False</code> is specified in the application parameters (by default <code>False</code>), returns only informational data:</p> <pre><code>{\n  \"model_info\": {\n    \"name\": \"MyFirstMLupModel\",\n    \"version\": \"1.0.0.0\",\n    \"type\": \"sklearn\",\n    \"columns\": null\n  },\n  \"web_app_info\": {\n    \"version\": \"1.0.0.0\",\n  },\n}\n</code></pre> <p>If <code>debug=True</code> is specified in the application parameters (the default is <code>False</code>_), it returns the entire config for the model and for the web application:</p> <pre><code>{\n  \"web_app_config\": {\n    \"host\": \"localhost\",\n    \"port\": 8009,\n    \"web_app_version\": \"1.0.0.0\",\n    \"column_validation\": false,\n    \"custom_column_pydantic_model\": null,\n    \"mode\": \"mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture\",\n    \"max_queue_size\": 100,\n    \"ttl_predicted_data\": 60,\n    \"ttl_client_wait\": 30.0,\n    \"min_batch_len\": 10,\n    \"batch_worker_timeout\": 1.0,\n    \"is_long_predict\": false,\n    \"show_docs\": true,\n    \"debug\": true,\n    \"throttling_max_requests\": null,\n    \"throttling_max_request_len\": null,\n    \"timeout_for_shutdown_daemon\": 3.0,\n    \"item_id_col_name\": \"mlup_item_id\"\n    },\n  \"model_config\": {\n    \"name\": \"MyFirstMLupModel\",\n    \"version\": \"1.0.0.0\",\n    \"type\": \"sklearn\",\n    \"columns\": null,\n    \"predict_method_name\": \"predict\",\n    \"auto_detect_predict_params\": true,\n    \"storage_type\": \"mlup.ml.storage.memory.MemoryStorage\",\n    \"binarization_type\": \"auto\",\n    \"use_thread_loop\": true,\n    \"max_thread_loop_workers\": null,\n    \"data_transformer_for_predict\": \"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\",\n    \"data_transformer_for_predicted\": \"mlup.ml.data_transformers.numpy_data_transformer.NumpyDataTransformer\",\n    \"dtype_for_predict\": null\n  }\n}\n</code></pre> <p>This allows you to debug and check what configuration is currently in use.</p>"},{"location":"web_app_api/#predict","title":"/predict","text":"<p>The model's prediction itself. This is where the client sends data and receives the model's prediction. The API method itself may change depending on the configuration.</p>"},{"location":"web_app_api/#post","title":"POST","text":"<p>If the parameter <code>auto_detect_predict_params=False</code> (default is <code>True</code>_), then the method accepts data in 1 parameter <code>data_for_predict</code>.</p> <p>Request body:</p> <pre><code>{\n  \"data_for_predict\": [\n    // features_for_obj1: List,\n    [1, 2, 3, 4, 5, 6],\n    // features_for_obj2: List,\n    [7, 6, 5, 4, 3, 2],\n    // ...\n  ]\n}\n</code></pre> <p>If <code>auto_detect_predict_params=True</code> (default <code>True</code>), then mlup analyzes the model and its predict method. And converts the arguments of the predict method of the model into parameters accepted by this API method. (_See Description of the application life cycle).</p> <p>For example, for scikit-learn models, the <code>predict</code> method has 1 argument <code>X: Any</code>. And then the API request will look like this:</p> <p>Request body:</p> <pre><code>{\n  \"X\": [\n    // features_for_obj1: List,\n    [1, 2, 3, 4, 5, 6],\n    // features_for_obj2: List,\n    [7, 6, 5, 4, 3, 2],\n    // ...\n  ]\n}\n</code></pre> <p>If a model has several arguments, and they were all parsed, then they all end up in the API and can be passed into the model\u2019s prediction method.</p> <p>Request body:</p> <pre><code>{\n  \"data_for_predict\": [\n    // features_for_obj1: List,\n    [1, 2, 3, 4, 5, 6],\n    // features_for_obj2: List,\n    [7, 6, 5, 4, 3, 2],\n    // ...\n  ],\n  \"check_something\": false,\n  // ...\n}\n</code></pre> <p>If no problems occurred, the method returns the prediction results.</p> <ul> <li>status_code: 200</li> </ul> <pre><code>{\n  \"predict_result\": [\n    // predict result for obj1\n    [1, 2, 3],\n    // predict result for obj2\n    [4, 2, 1]\n    // ...\n  ]\n}\n</code></pre> <p>In addition, the <code>X-Predict-id</code> header is always returned, even if an error occurs.</p> <p>The architecture may change the response body returned by this method. For example, for the architecture <code>mlup.web.architecture.worker_and_queue.WorkerAndQueueArchitecture</code> and <code>mlup.web.architecture.batching.BatchingSingleProcessArchitecture</code>, if the <code>is_long_predict=True</code> configuration option is enabled, this method will return <code>predict_id</code>. You can use it to pick up the results later. (See Web app architectures).</p> <pre><code>{\n  \"predict_id\": \"7d9bea07-7505-45ec-a700-40417842025b\"\n}\n</code></pre>"},{"location":"web_app_api/#get-predictpredict_id","title":"/get-predict/{predict_id}","text":"<p>This API method is only used in the <code>mlup.web.architecture.worker_and_queue.WorkerAndQueueArchitecture</code> and <code>mlup.web.architecture.batching.BatchingSingleProcessArchitecture</code> architectures. If the model takes a long time to make a prediction, or the client wants to pick up the prediction results later, he can set the <code>is_long_predict=True</code> configuration parameter and go for the results later. (See Web app architectures).</p>"},{"location":"web_app_api/#get_1","title":"GET","text":"<p>To get results, you need to send the required <code>predict_id</code> parameter to the URL.</p> <p>If results for such a <code>predict_id</code> are found, they will be returned with status_code=200.</p> <pre><code>// results for predict_id = \"7d9bea07-7505-45ec-a700-40417842025b\"\n{\n  \"predict_result\": [\n    // predict result for obj1\n    [1, 2, 3],\n    // predict result for obj2\n    [4, 2, 1]\n    // ...\n  ]\n}\n</code></pre> <p>And if the prediction results were not found, then after <code>ttl_client_wait</code> has elapsed from the start of the request time, the application will return status_code 408.</p>"},{"location":"web_app_api/#api-errors","title":"API Errors","text":"<p>All errors are in errors.</p> <p>All error APIs have a common response format. Response format:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\n        \"string\"\n      ],\n      \"msg\": \"string\",\n      \"type\": \"string\"\n    }\n  ],\n  \"predict_id\": \"string\"\n}\n</code></pre>"},{"location":"web_app_api/#422","title":"422","text":"<p>Validation error. If such an error is returned, it means that the sent data was not validated and the request was not accepted.</p> <p>Example answer:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\n        \"input_data\",\n        0\n      ],\n      \"msg\": \"value is not a valid dict\",\n      \"type\": \"type_error.dict\"\n    },\n    {\n      \"loc\": [\n        \"input_data\",\n        1,\n        \"col1\"\n      ],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ],\n  \"predict_id\": \"15eb8236-02bf-4658-9c04-cc53f37afdc3\"\n}\n</code></pre>"},{"location":"web_app_api/#429","title":"429","text":"<p>This error occurs when the request is not throttled.</p> <p>Example answer:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [], \n      \"msg\": \"Max requests in app. Please try again later.\", \n      \"type\": \"throttling_error\"\n    }\n  ],\n  \"predict_id\": \"15eb8236-02bf-4658-9c04-cc53f37afdc3\"\n}\n</code></pre>"},{"location":"web_app_api/#500","title":"500","text":"<p>Error during model prediction or data transformation.</p> <p>Example answer:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [],\n      \"msg\": \"[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'Gather_2' Status Message: indices element out of data bounds, idx=123124124124 must be within the inclusive range [-250002,250001]\",\n      \"type\": \"predict_error\"\n    }\n  ],\n  \"predict_id\": \"0c4b7afe-032e-442f-b265-7773d9d78580\"\n}\n</code></pre>"},{"location":"web_app_api/#api_1","title":"\u0421\u0432\u043e\u0439 API","text":"<p>As shown in the life_cycle.md section, you can completely customize a web application after it has been created.</p> <p>Including, you can add new or even change existing API methods.</p>"},{"location":"web_app_architectures/","title":"Web app architectures","text":"<p>The mlup web application has several architectures: * mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture; * mlup.web.architecture.worker_and_queue.WorkerAndQueueArchitecture; * mlup.web.architecture.batching.BatchingSingleProcessArchitecture;</p> <p>You can change the architecture using the <code>mode</code> configuration parameter (See \"Description of the configuration file\"). <code>mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture</code> is set in the default config.</p> <p>As described in \"Description of the application life cycle\", loading a web application is only possible when <code>mlup.ml</code> has already been loaded into memory.</p> <p>The architecture requires initialization just like a web application. This happens inside <code>mlup.UP.web.load()</code>, after the web application itself has been fully initialized. In fact, the architecture must support an interface of several methods:</p> <pre><code>from typing import Any, Dict\nimport mlup\nfrom mlup.constants import WebAppArchitecture\nfrom mlup.web.architecture.base import BaseWebAppArchitecture\n\nclass Architecture(BaseWebAppArchitecture):\n    ml: mlup.MLupModel\n    type: WebAppArchitecture\n\n    def __init__(self, **configs):\n        self.archi_conf_name = configs.pop('archi_conf_name')\n        # ...\n\n    def load(self):\n        pass\n\n    async def run(self):\n        pass\n\n    @property\n    def is_running(self) -&gt; bool:\n        pass\n\n    async def stop(self):\n        pass\n\n    async def predict(self, data_for_predict: Dict, predict_id: str) -&gt; Any:\n        pass\n</code></pre> <p>The constructor contains all configs from the web application configuration.</p> <p>The <code>load</code> method is called when the architecture is initialized, the last step in initializing a web application. This method creates everything necessary for the architecture to work - all internal structures and objects.</p> <p>The <code>run</code> method is called when the application starts using the lifespan FastAPI mechanism (see FastAPI lifespan docs). The <code>stop</code> method is called when the application is terminated using the FastAPI lifespan mechanism (see FastAPI lifespan docs).</p> <pre><code>@asynccontextmanager\nasync def _lifespan(app: FastAPI):\n    # Startup web app code\n    await architecture.run()\n    yield\n    # Shutdown web app code\n    await architecture.stop()\n</code></pre> <p>The <code>predict</code> method is called every time a new prediction request comes from the user. The <code>data_for_predict</code> argument contains already validated data in the same format in which it was received in the web request. The <code>predict_id</code> argument includes the unique generated id of the predict request. It can be used to uniquely identify the data included in the request.</p> <p>The <code>predict</code> method itself calls <code>mlup.UP.ml.predict</code> at the right time.</p>"},{"location":"web_app_architectures/#mlupwebarchitecturedirectly_to_predictdirectlytopredictarchitecture","title":"mlup.web.architecture.directly_to_predict.DirectlyToPredictArchitecture","text":"<p>This is a simple web application architecture - a direct call to model prediction.</p> <p>When several parallel requests arrive, each of them, independently of each other, will call <code>web.UP.ml.predict</code>. This can lead to various problems: * Using more resources. * Slowdown of prediction and web application. * Locks if the model cannot simultaneously work in parallel.</p> <p>This architecture is well suited for fast models that can process multiple requests per second.</p> <p></p>"},{"location":"web_app_architectures/#initialization-of-the-architecture","title":"Initialization of the architecture","text":"<p>This architecture does not require any initialization.</p>"},{"location":"web_app_architectures/#request-lifecycle","title":"Request Lifecycle","text":"<p>The request is processed immediately and directly - <code>mlup.UP.ml.predict</code> is called.</p> <pre><code>async def predict(self, data_for_predict: Dict, predict_id: str) -&gt; Any:\n    return await self.ml.predict(**data_for_predict)\n</code></pre>"},{"location":"web_app_architectures/#mlupwebarchitectureworker_and_queueworkerandqueuearchitecture","title":"mlup.web.architecture.worker_and_queue.WorkerAndQueueArchitecture","text":"<p>This architecture involves running the machine learning model in a separate thread, thereby not blocking the main thread of the web application.</p> <p>Predictions are asynchronous: * WEB: Place the request in the prediction request queue. * WORKER: The model rakes up this queue and runs a predictor on the data from the queue one by one. * WORKER: The model puts the prediction results into the result store. * WEB: The query waits for its result by <code>predict_id</code> to appear in the result store.</p> <p>This approach allows you to control concurrent user access to the model resource. The model will always process only 1 request while the others are in the queue.</p> <p>To configure queue sizes and response prediction lifetimes, there are configuration parameters: * <code>max_queue_size: int</code> - maximum queue size for prediction requests. * <code>ttl_predicted_data: int</code> - maximum lifetime of the prediction result in seconds.   If the client does not wait for the prediction results, its prediction is no longer needed and should be removed from the results store.   This setting controls how long after the results are deleted if the client has not collected them.   The countdown starts from the moment the result is saved in the results storage. * <code>is_long_predict: bool</code> - enable an additional API method in which you can retrieve the prediction result.   When the prediction takes a long time or the client wants to retrieve the prediction results later, he can do this in a separate API method.   This parameter adds an API method <code>GET: /get-predict/{predict_id}</code>, in which you can get prediction results if they are in the results store.   See \"Web app API\". * <code>ttl_client_wait: float</code> - the maximum time to wait for client results in the <code>GET: /get-predict/{predict_id}</code> method in seconds.   See \"Web app API\".</p> <p></p>"},{"location":"web_app_architectures/#initialization-of-the-architecture_1","title":"Initialization of the architecture","text":"<p>For the architecture to work, you need: * Queue for prediction requests. * Storage of prediction results with TTL. * Thread for model work.</p> <p>All this is created at the time of initialization. Since mlup works in an asynchronous paradigm, the model prediction worker is launched as <code>asyncio.Task</code>. This allows you not to block the web application during model prediction, within one process.</p>"},{"location":"web_app_architectures/#request-lifecycle_1","title":"Request Lifecycle","text":"<p>The processing of the request with all the nuances is described here. Request path: * A validated request is included in the architecture. * The architecture places the request data in a prediction queue. * When a request waits its turn, the worker extracts the <code>predict_id</code> from it and sends the user data to the model. * The request is digested by the model and it makes a prediction. The worker combines the prediction results with the previously extracted <code>predict_id</code> and stores them in the results storage. * If an error occurs during prediction, then the worker will store this error in the results store under the same conditions as the results and with the same TTL. * The client retrieves the prediction result from the result store. This could be the result of a prediction or an error.</p> <p>The architecture has a method <code>_get_result_from_storage</code>, which time <code>ttl_client_wait</code> tries to find the prediction result in the storage by <code>predict_id</code>. If an error occurs during prediction, the worker will store this error in the results store instead of the prediction result. And when the <code>_get_result_from_storage</code> method reads the results and sees an error, it will return an error to the client with the appropriate response code.</p> <p>If <code>is_long_predict=False</code>, then the architecture, after adding a request to the prediction queue, immediately calls this method and waits exactly <code>ttl_client_wait</code> for the results. If during <code>ttl_client_wait</code> the prediction results have not appeared in the storage, the web application responds to the client with a 408 response code and will not be able to get any more results. The result will be removed from the storage after adding it there via <code>ttl_predicted_data</code>.</p> <p>If <code>is_long_predict=True</code>, then the architecture immediately responds with JSON <code>{\"predict_id\": \"your_predict_id\"}</code> after adding a request to the prediction queue. After which, the client, at a convenient time, can make a request to <code>GET: /get-predict/{predict_id}</code> for the results. The handler for this request simply calls <code>_get_result_from_storage</code> and returns what this method returned.</p> <pre><code>async def get_predict_result(predict_id: str):\n    return await _get_result_from_storage(predict_id)\n</code></pre> <p>The client can make as many requests to <code>GET: /get-predict/{predict_id}</code>. But prediction results or prediction error can only be obtained once for one <code>predict_id</code>. Once received by the client, the results will be deleted from the results storage.</p>"},{"location":"web_app_architectures/#mlupwebarchitecturebatchingbatchingsingleprocessarchitecture","title":"mlup.web.architecture.batching.BatchingSingleProcessArchitecture","text":"<p>This architecture is very similar to <code>mlup.web.architecture.worker_and_queue.WorkerAndQueueArchitecture</code>. But <code>mlup.web.architecture.worker_and_queue.WorkerAndQueueArchitecture</code> sends 1 request for prediction sequentially. While <code>mlup.web.architecture.batching.BatchingSingleProcessArchitecture</code> combines several requests into one batch and sends this batch for prediction.</p> <p>This architecture is very convenient to use when your model spends approximately the same time predicting one and several objects. For example, it takes ~1 second to predict 1 object, and ~1.1 second to predict many objects.</p> <p>This architecture involves running the machine learning model in a separate thread, thereby not blocking the main thread of the web application. * WEB: Place the request in the prediction request queue. * WORKER: The model clears this queue, combines several requests into one batch and runs a predictor for the batch. * WORKER: The model puts the batch prediction results into the results store. * WEB: The query waits for its result by <code>predict_id</code> to appear in the result store.</p> <p>This approach allows you to control concurrent user access to the model resource. At the same time, the model can simultaneously process several requests at once.</p> <p>The batch has two restrictions: * <code>min_batch_len: int</code> - the minimum batch size that is needed to start prediction. The worker will run a prediction on the model of the current batch if this batch exceeds the size specified in this parameter. * <code>batch_worker_timeout: float</code> - maximum time for creating a batch. The worker will start a prediction for the current batch model if the batch generation time has reached this value. Even if the batch size has not reached <code>min_batch_len</code>.</p> <p>Batching reads <code>batch_worker_timeout</code> time requests from the prediction request queue. If, after adding the next request to the batch, the batch size exceeds <code>min_batch_len</code>, the worker will send the batch for prediction. If during the <code>batch_worker_timeout</code> time the batch has not reached a size exceeding <code>min_batch_len</code>, the worker sends it for prediction in the form in which it is.</p> <p>The <code>batch_worker_timeout</code> timeout starts after the first request is added to the batch. After each batch prediction, the batch is reset and re-assembled. Accordingly, the <code>batch_worker_timeout</code> time is reset to zero.</p> <p>We can only regulate the minimum batch size, and cannot regulate the maximum batch size. Since in one request more than one object for prediction can come, but several, a worker can read 1 request from the queue and receive several objects for prediction at once. Therefore, you cannot adjust the batch size from above - ~~max_batch_len~~. It is possible to read from the prediction request queue more objects to predict than ~~max_batch_len~~. For example, ~~max_batch_len~~=10`, and two requests came with 9 and 3 objects. Then the worker, having read the first request from the queue, will not get it for the batch. It will read the second request and the batch will be larger than it should be. Therefore, it will not be able to send data for prediction.</p> <p>In addition to the <code>batch_worker_timeout</code> and <code>min_batch_len</code> batching parameters, there are configuration parameters for configuring the batching architecture: * <code>max_queue_size: int</code> - maximum queue size for prediction requests. * <code>ttl_predicted_data: int</code> - maximum lifetime of the prediction result in seconds.    If the client does not wait for the prediction results, its prediction is no longer needed and should be removed from the results store.   This setting controls how long after the results are deleted if the client has not collected them.   The countdown starts from the moment the result is saved in the results storage. * <code>is_long_predict: bool</code> - enable an additional API method in which you can retrieve the prediction result.   When the prediction takes a long time or the client wants to retrieve the prediction results later, he can do this in a separate API method.   This parameter adds an API method <code>GET: /get-predict/{predict_id}</code>, in which you can get prediction results if they are in the results store.   See \"Web app API\". * <code>ttl_client_wait: float</code> - the maximum time to wait for client results in the <code>GET: /get-predict/{predict_id}</code> method in seconds.   See \"Web app API\".</p> <p></p>"},{"location":"web_app_architectures/#initialization-of-the-architecture_2","title":"Initialization of the architecture","text":"<p>For this architecture to work, you need: * Queue for prediction requests. * Storage of prediction results with TTL. * Thread for model work.</p> <p>All this is created at the time of initialization. Since mlup works in an asynchronous paradigm, the model prediction worker is launched as <code>asyncio.Task</code>. This allows you not to block the web application during model prediction, within one process.</p>"},{"location":"web_app_architectures/#request-lifecycle_2","title":"Request Lifecycle","text":"<p>The processing of the request with all the nuances is described here. Request path: * A validated request is included in the architecture. * The architecture places the request data in a prediction queue. * When a request waits its turn, the worker takes the <code>predict_id</code> from it and adds the user data to the batch. * The batch is formed as a Python List, where new requests from the queue are added to the end of the list. * When a batch is formed by size or collection time, it is sent to the model for prediction. * The request is digested by the model and it makes a prediction. It is important that the data transformers and the model itself give prediction results for objects in the same order in which the data entered the model.    In most cases, models behave this way.   The worker divides the prediction results by <code>predict_id</code> and stores them in the results storage. * If an error occurs during prediction, then the worker will store this error in the results store under the same conditions as the results and with the same TTL. * The client retrieves the prediction result from the result store. This could be the result of a prediction or an error.</p> <p>Let's take 3 queries: 2, 4, 1 objects in each, respectively <code>[[1, 2], [3, 4, 5, 6], [7]]</code>. A batch of size 7 is assembled from them in the order in which the requests came and the objects in them are <code>[1, 2, 3, 4, 5, 6, 7]</code>. When the model made its predictions, it returned the same array of 7 responses: <code>[\"ans1\", \"ans2\", \"ans3\", \"ans4\", \"ans5\", \"ans6\", \"ans7\"]</code>. Batching will dissolve the response array into as many arrays as were involved in the request batch - 3 <code>[[\"ans1\", \"ans2\"], [\"ans3\", \"ans4\", \"ans5\", \"ans6\"], [\"ans7\"]]</code>.</p> <p>The architecture has a method <code>_get_result_from_storage</code>, which time <code>ttl_client_wait</code> tries to find the prediction result in the storage by <code>predict_id</code>. If an error occurs during prediction, the worker will store this error in the results store instead of the prediction result. And when the <code>_get_result_from_storage</code> method reads the results and sees an error, it will return an error to the client with the appropriate response code. Since multiple queries are involved in the prediction at the same time, an error will be returned for each of these queries.</p> <p>If <code>is_long_predict=False</code>, then the architecture, after adding a request to the prediction queue, immediately calls this method and waits exactly <code>ttl_client_wait</code> for the results. If during <code>ttl_client_wait</code> the prediction results have not appeared in the storage, the web application responds to the client with a 408 response code and will not be able to get any more results. The result will be removed from the storage after adding it there via <code>ttl_predicted_data</code>.</p> <p>If <code>is_long_predict=True</code>, then the architecture immediately responds with JSON <code>{\"predict_id\": \"your_predict_id\"}</code> after adding a request to the prediction queue. After which, the client, at a convenient time, can make a request to <code>GET: /get-predict/{predict_id}</code> for the results. The handler for this request simply calls <code>_get_result_from_storage</code> and returns what this method returned.</p> <pre><code>async def get_predict_result(predict_id: str):\n    return await _get_result_from_storage(predict_id)\n</code></pre> <p>The client can make as many requests to <code>GET: /get-predict/{predict_id}</code>. But prediction results or prediction error can only be obtained once for one <code>predict_id</code>. Once received by the client, the results will be deleted from the result storage.</p>"},{"location":"web_app_architectures/#custom-architecture","title":"Custom architecture","text":"<p>If the capabilities of mlup web architectures are not enough for you, you can write your own web architecture.</p> <p>The web architecture interface is:</p> <pre><code># my_module.py\nfrom typing import Any, Dict\nimport mlup\nfrom mlup.constants import WebAppArchitecture\nfrom mlup.web.architecture.base import BaseWebAppArchitecture\n\nclass Architecture(BaseWebAppArchitecture):\n    ml: mlup.MLupModel\n    type: WebAppArchitecture\n\n    def __init__(self, **configs):\n        self.archi_conf_name = configs.pop('archi_conf_name')\n        # ...\n\n    def load(self):\n        pass\n\n    async def run(self):\n        pass\n\n    @property\n    def is_running(self) -&gt; bool:\n        pass\n\n    async def stop(self):\n        pass\n\n    async def predict(self, data_for_predict: Dict, predict_id: str) -&gt; Any:\n        pass\n</code></pre> <p>And specify the path to import your module in <code>mode</code>: <code>my_module.Architecture</code>.</p> <p>IMPORTANT: a web architecture written independently must be available for import on the server on which you run the mlup application.</p> <p>The easiest way to do this is to create your own python library with your web architecture and other useful classes and install it on your server along with the pymlup library.</p>"}]}